services:
  triton:
    restart: $RESTART_POLICY
    # This is the last version of Triton that supports natively Tensorflow SavedModel
    # format.
    image: nvcr.io/nvidia/tritonserver:25.02-py3
    ports:
      - ${TRITON_EXPOSE_HTTP:-5503}:8000
      - ${TRITON_EXPOSE_GRPC:-5504}:8001
      - ${TRITON_EXPOSE_METRICS:-5505}:8002
    volumes:
      - ${TRITON_MODELS_DIR:-./triton}:/models
    # We need to add nvidia_entrypoint.sh for the GPU to be correctly detected
    # by Triton
    # We use explicit model control mode to be able to load/unload model dynamically
    # without having to restart Triton server
    # See
    # https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md
    # for more information
    entrypoint: "/opt/nvidia/nvidia_entrypoint.sh tritonserver --model-repository=/models --model-control-mode=explicit --load-model=*"
    mem_limit: 30g
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
