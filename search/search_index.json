{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Robotoff Robotoff is a service that generates predictions from product data such as images, text, and metadata. These predictions can individually or collectively be refined into actionable suggestions known as insights. Insights serve as potential updates to the Open Food Facts database, either automatically applied when highly confident or validated manually by human annotators. These insights cover a growing set of product facts, including: the product category, weight, brand, packager codes and expiration date some of its labels abusive pictures (selfies) rotated pictures Robotoff provides an API to: Fetch insights Annotate an insight (accept or reject) Once generated, the insights can be applied automatically, or after a manual validation if necessary. A scheduler regularly marks insights for automatic annotation and sends the update to Open Food Facts. A detailed description of how predictions and insights work is available here . Robotoff works together with Product Opener , the Core server of Open Food Facts (in Perl, which can also be installed locally using Docker) and the Open Food Facts apps (which can work with your local instance after enabling dev mode) \ud83d\udcda Documentation: https://openfoodfacts.github.io/robotoff Source code: https://github.com/openfoodfacts/robotoff \ud83c\udf4a Open Food Facts: https://world.openfoodfacts.org Overview To get a better understanding on how Robotoff works, go to Architecture . If you want to help, go to Contributing . Robotoff can be used as... an online API a CLI tool a Python package If you need to deploy or maintain Robotoff, Maintenance is the way to go. \ud83d\udd27 Development API Specification Linting Robotoff uses Spectral to lint the OpenAPI specification file located at docs/references/api.yml . This ensures the API documentation follows best practices and remains consistent. Running API Linting All linting commands use Docker, so you don't need to install Spectral locally: # Lint the API specification with pretty formatting make api-lint # Check the API specification and fail on errors (used in CI) make api-lint-check Configuration The linting rules are configured in .spectral.yml at the root of the repository. The configuration extends the standard OpenAPI ruleset with custom rules specific to the Robotoff API. CI Integration API linting runs automatically on GitHub Actions when: - Changes are made to docs/references/api.yml - Changes are made to .spectral.yml The workflow will fail if there are any linting errors, ensuring the API specification maintains high quality. NOTE: This documentation tries to follow as much as possible the documentation system from Di\u00e1taxis . \ud83d\udc69\u200d\u2696\ufe0f Licence Robotoff is licensed under the AGPLv3. \ud83d\udcc6 Weekly meetings We e-meet every first Tuesday of each month, at 11:00 Paris Time (10:00 London Time, 15:30 IST, 02:00 AM PT) Video call link: https://meet.google.com/qvv-grzm-gzb Join by phone: https://tel.meet/qvv-grzm-gzb?pin=9965177492770 Add the Event to your Calendar by adding the Open Food Facts community calendar to your calendar Weekly Agenda : please add the Agenda items as early as you can. Make sure to check the Agenda items in advance of the meeting, so that we have the most informed discussions possible. The meeting will handle Agenda items first, and if time permits, collaborative bug triage. We strive to timebox the core of the meeting (decision making) to 30 minutes, with an optional free discussion/live debugging afterwards. We take comprehensive notes in the Weekly Agenda of agenda item discussions and of decisions taken. Contributors","title":"Robotoff"},{"location":"#robotoff","text":"Robotoff is a service that generates predictions from product data such as images, text, and metadata. These predictions can individually or collectively be refined into actionable suggestions known as insights. Insights serve as potential updates to the Open Food Facts database, either automatically applied when highly confident or validated manually by human annotators. These insights cover a growing set of product facts, including: the product category, weight, brand, packager codes and expiration date some of its labels abusive pictures (selfies) rotated pictures Robotoff provides an API to: Fetch insights Annotate an insight (accept or reject) Once generated, the insights can be applied automatically, or after a manual validation if necessary. A scheduler regularly marks insights for automatic annotation and sends the update to Open Food Facts. A detailed description of how predictions and insights work is available here . Robotoff works together with Product Opener , the Core server of Open Food Facts (in Perl, which can also be installed locally using Docker) and the Open Food Facts apps (which can work with your local instance after enabling dev mode) \ud83d\udcda Documentation: https://openfoodfacts.github.io/robotoff Source code: https://github.com/openfoodfacts/robotoff \ud83c\udf4a Open Food Facts: https://world.openfoodfacts.org","title":"Robotoff"},{"location":"#overview","text":"To get a better understanding on how Robotoff works, go to Architecture . If you want to help, go to Contributing . Robotoff can be used as... an online API a CLI tool a Python package If you need to deploy or maintain Robotoff, Maintenance is the way to go.","title":"Overview"},{"location":"#development","text":"","title":"\ud83d\udd27 Development"},{"location":"#api-specification-linting","text":"Robotoff uses Spectral to lint the OpenAPI specification file located at docs/references/api.yml . This ensures the API documentation follows best practices and remains consistent.","title":"API Specification Linting"},{"location":"#running-api-linting","text":"All linting commands use Docker, so you don't need to install Spectral locally: # Lint the API specification with pretty formatting make api-lint # Check the API specification and fail on errors (used in CI) make api-lint-check","title":"Running API Linting"},{"location":"#configuration","text":"The linting rules are configured in .spectral.yml at the root of the repository. The configuration extends the standard OpenAPI ruleset with custom rules specific to the Robotoff API.","title":"Configuration"},{"location":"#ci-integration","text":"API linting runs automatically on GitHub Actions when: - Changes are made to docs/references/api.yml - Changes are made to .spectral.yml The workflow will fail if there are any linting errors, ensuring the API specification maintains high quality. NOTE: This documentation tries to follow as much as possible the documentation system from Di\u00e1taxis .","title":"CI Integration"},{"location":"#licence","text":"Robotoff is licensed under the AGPLv3.","title":"\ud83d\udc69\u200d\u2696\ufe0f Licence"},{"location":"#weekly-meetings","text":"We e-meet every first Tuesday of each month, at 11:00 Paris Time (10:00 London Time, 15:30 IST, 02:00 AM PT) Video call link: https://meet.google.com/qvv-grzm-gzb Join by phone: https://tel.meet/qvv-grzm-gzb?pin=9965177492770 Add the Event to your Calendar by adding the Open Food Facts community calendar to your calendar Weekly Agenda : please add the Agenda items as early as you can. Make sure to check the Agenda items in advance of the meeting, so that we have the most informed discussions possible. The meeting will handle Agenda items first, and if time permits, collaborative bug triage. We strive to timebox the core of the meeting (decision making) to 30 minutes, with an optional free discussion/live debugging afterwards. We take comprehensive notes in the Weekly Agenda of agenda item discussions and of decisions taken.","title":"\ud83d\udcc6 Weekly meetings"},{"location":"#contributors","text":"","title":"Contributors"},{"location":"explanations/db-migrations/","text":"Database migrations We use peewee_migrate to handle our SQL database migrations. Create a migration You should create a migration if you update the SQL database schema. To create a new migration, use: make robotoff-cli args='create-migration MIGRATION_NAME --auto' You should use an identifiable name (such as \"add_bounding_box\") to the migration. The --auto flag is used to ask peewee_migrate to scan all table definitions in the source code and create the migration file automatically. You can skip it if you want peewee_migrate to create a blank migration file. Apply the migration To apply all pending migrations, use: make migrate-db What if I had a previous installation of Robotoff with an initialized DB? You should then create yourself the migration table: CREATE TABLE IF NOT EXISTS \"migratehistory\" ( \"id\" SERIAL NOT NULL PRIMARY KEY , \"name\" VARCHAR ( 255 ) NOT NULL , \"migrated_at\" TIMESTAMP NOT NULL ) Then add manually the the initial migration file in database: INSERT INTO \"migratehistory\" VALUES ( 1 , '001_initial' , CURRENT_TIMESTAMP ); Then you can apply remaining migrations with make migrate-db .","title":"Database migrations"},{"location":"explanations/db-migrations/#database-migrations","text":"We use peewee_migrate to handle our SQL database migrations.","title":"Database migrations"},{"location":"explanations/db-migrations/#create-a-migration","text":"You should create a migration if you update the SQL database schema. To create a new migration, use: make robotoff-cli args='create-migration MIGRATION_NAME --auto' You should use an identifiable name (such as \"add_bounding_box\") to the migration. The --auto flag is used to ask peewee_migrate to scan all table definitions in the source code and create the migration file automatically. You can skip it if you want peewee_migrate to create a blank migration file.","title":"Create a migration"},{"location":"explanations/db-migrations/#apply-the-migration","text":"To apply all pending migrations, use: make migrate-db","title":"Apply the migration"},{"location":"explanations/db-migrations/#what-if-i-had-a-previous-installation-of-robotoff-with-an-initialized-db","text":"You should then create yourself the migration table: CREATE TABLE IF NOT EXISTS \"migratehistory\" ( \"id\" SERIAL NOT NULL PRIMARY KEY , \"name\" VARCHAR ( 255 ) NOT NULL , \"migrated_at\" TIMESTAMP NOT NULL ) Then add manually the the initial migration file in database: INSERT INTO \"migratehistory\" VALUES ( 1 , '001_initial' , CURRENT_TIMESTAMP ); Then you can apply remaining migrations with make migrate-db .","title":"What if I had a previous installation of Robotoff with an initialized DB?"},{"location":"explanations/interactions-product-opener/","text":"Interactions with Product Opener Robotoff mainly interacts with Product Opener (Open Food Facts backend service) through three means: it receives updates through Redis when a product is created/modified/deleted or when an image is uploaded it has a direct read/write access to the underlying MongoDB that powers Open Food Facts it calls Product Opener API to update/delete products and images and to fetch some information Redis notification Product update A new event is published on the product_updates_off Redis stream whenever a product is updated or deleted. This event contains the following fields: code : the barcode of the product action : the action performed, either updated or deleted . Image upload uses the updated action. flavor : the flavor ( off , obf , opff , off_pro ) user_id : the user ID that performed the action comment : the user comment associated with the action diffs : the differences between the old and new product data product_type : the product type ( food , petfood , beauty ,...) After receiving an updated event, Robotoff does the following 1 : predict categories and import category predictions/insights generate and import predictions/insights from product name (regex/flashtext-based) refresh all insights for this product (i.e: import all predictions again, and update insight table accordingly) Image uploaded Uploaded images trigger an updated event, with the following diffs: { \"uploaded_images\" : { \"add\" : [ \"1\" ] } } with 1 being the ID of the uploaded image. After receiving this event, Robotoff does the following 2 : save the image metadata in the Image DB table import insights from the image run logo detection pipeline on the image Insight import includes the following steps: extract and import regex/flashtext predictions and insights from the image OCR run the nutriscore object detection model, if the \"NUTRISCORE\" string was detected in the OCR text. This is to avoid wasting compute resources and preventing false positive results. run logo detection pipeline: detect logos using the object detection models, generate embeddings from logo crops and index them in the Elasticsearch ANN index. MongoDB interaction Robotoff often needs to access the latest information about product. As the API is sometimes unresponsive, Robotoff has a direct access to the MongoDB. It is used to fetch the product data in the products collection. Robotoff also has a write access, that allows it to add facets dynamically. This process is done every night; only the en:missing-nutrition-facts-fibers-present-on-photos quality facet is currently added by Robotoff. Product Opener API Robotoff also interacts directly with Product Opener through it's API. Everytime an insight is annotated ( annotation=1 ), either manually or automatically, Robotoff calls Product Opener API to perform the associated action (update the product, delete an image,...). Robotoff also depends on static resources from Product Opener: taxonomy files (ex: https://static.openfoodfacts.org/data/taxonomies/categories.full.json ) which are downloaded and cached in RAM the JSONL dump: https://static.openfoodfacts.org/data/openfoodfacts-products.jsonl.gz . This is used to scan the entire database and update predictions/insights without having to do a full MongoDB scan. images/OCR files see update_insights_job function in robotoff.workers.tasks.product_updated \u21a9 see run_import_image_job function in robotoff.workers.tasks.import_image \u21a9","title":"Interactions with Product Opener"},{"location":"explanations/interactions-product-opener/#interactions-with-product-opener","text":"Robotoff mainly interacts with Product Opener (Open Food Facts backend service) through three means: it receives updates through Redis when a product is created/modified/deleted or when an image is uploaded it has a direct read/write access to the underlying MongoDB that powers Open Food Facts it calls Product Opener API to update/delete products and images and to fetch some information","title":"Interactions with Product Opener"},{"location":"explanations/interactions-product-opener/#redis-notification","text":"","title":"Redis notification"},{"location":"explanations/interactions-product-opener/#product-update","text":"A new event is published on the product_updates_off Redis stream whenever a product is updated or deleted. This event contains the following fields: code : the barcode of the product action : the action performed, either updated or deleted . Image upload uses the updated action. flavor : the flavor ( off , obf , opff , off_pro ) user_id : the user ID that performed the action comment : the user comment associated with the action diffs : the differences between the old and new product data product_type : the product type ( food , petfood , beauty ,...) After receiving an updated event, Robotoff does the following 1 : predict categories and import category predictions/insights generate and import predictions/insights from product name (regex/flashtext-based) refresh all insights for this product (i.e: import all predictions again, and update insight table accordingly)","title":"Product update"},{"location":"explanations/interactions-product-opener/#image-uploaded","text":"Uploaded images trigger an updated event, with the following diffs: { \"uploaded_images\" : { \"add\" : [ \"1\" ] } } with 1 being the ID of the uploaded image. After receiving this event, Robotoff does the following 2 : save the image metadata in the Image DB table import insights from the image run logo detection pipeline on the image Insight import includes the following steps: extract and import regex/flashtext predictions and insights from the image OCR run the nutriscore object detection model, if the \"NUTRISCORE\" string was detected in the OCR text. This is to avoid wasting compute resources and preventing false positive results. run logo detection pipeline: detect logos using the object detection models, generate embeddings from logo crops and index them in the Elasticsearch ANN index.","title":"Image uploaded"},{"location":"explanations/interactions-product-opener/#mongodb-interaction","text":"Robotoff often needs to access the latest information about product. As the API is sometimes unresponsive, Robotoff has a direct access to the MongoDB. It is used to fetch the product data in the products collection. Robotoff also has a write access, that allows it to add facets dynamically. This process is done every night; only the en:missing-nutrition-facts-fibers-present-on-photos quality facet is currently added by Robotoff.","title":"MongoDB interaction"},{"location":"explanations/interactions-product-opener/#product-opener-api","text":"Robotoff also interacts directly with Product Opener through it's API. Everytime an insight is annotated ( annotation=1 ), either manually or automatically, Robotoff calls Product Opener API to perform the associated action (update the product, delete an image,...). Robotoff also depends on static resources from Product Opener: taxonomy files (ex: https://static.openfoodfacts.org/data/taxonomies/categories.full.json ) which are downloaded and cached in RAM the JSONL dump: https://static.openfoodfacts.org/data/openfoodfacts-products.jsonl.gz . This is used to scan the entire database and update predictions/insights without having to do a full MongoDB scan. images/OCR files see update_insights_job function in robotoff.workers.tasks.product_updated \u21a9 see run_import_image_job function in robotoff.workers.tasks.import_image \u21a9","title":"Product Opener API"},{"location":"explanations/predictions/","text":"Predictions and insights Robotoff purpose is to generate predictions about Open Food Facts products from various sources: images, image OCRs, product meta data,... A complete list of predictions types can be found in robotoff.prediction.types . The most common ones are brand , label , category ,... All predictions are stored in the PostgreSQL database in the prediction table. Predictions most interesting fields are the following: barcode : barcode of the product (string) type : prediction type value : the predicted untaxonomized value (ex: carrefour for brand prediction type), optional value_tag : the predicted taxonomized value (ex: en:organic for label prediction type), optional source_image : the path of the image the prediction was generated from (ex: /847/000/700/5117/1.jpg ). May be null, it is mainly provided for OCR and object detection-based predictions. automatic_processing : a boolean indicating whether we're confident enough in the prediction to apply it automatically in Open Food Facts without human supervision. This does not mean it will indeed be applied automatically, please refer to the import mechanism description below to know how automatic processing works. data : a JSON structure containing prediction data. It either complements value and value_tag with additional data or contains the full prediction data. predictor : name of the predictor that generated the prediction. Every insight type has its own predictor s, but most common ones are: universal-logo-detector for predictions generated by the nearest-neighbors logo detector flashtext for all predictions generated using flashtext library regex for all predictions generated using simple regex predictor_version : this is a version ID that is used to know when to replace predictions in database by new ones during import, and when to keep them. It is either an incrementing integer (for regex-based predictions) or the version of the model that generated the predictions. From these predictions, we generate insights . Insights are refined predictions about the product that are directly actionable: if the insight is validated by a human, we can update the product accordingly. For example, we may have the following predictions on a (french) organic product: prediction 1: type : label value_tag : en:organic prediction 2: type : label value_tag : fr:ab-agriculture-biologique As fr:ab-agriculture-biologique is a children of en:organic in the label taxonomy, we only generate a label insight with value_tag=fr:ab-agriculture-biologique . Furthermore, we don't generate any insight if the product already has the fr:ab-agriculture-biologique label. This is the different between insights and predictions : predictions can be viewed as raw data, and insights as actionable data. Most insight types are generated directly from their corresponding prediction types (ex: brand , label ,...). However, an insight type can require prediction of several types: this allow the generation of complex insight types. Insights can either be applied automatically or require a human annotation. Insights are saved in DB in the product_insight table. Insight fields are a superset of prediction fields \u2014 the most interesting additional fields are: annotation : either 0 (incorrect insight), 1 (correct), or -1 (invalid). Automatically applied insights have annotation=1 automatic_processing : if True, the insight will be applied automatically completed_at : timestamp of annotation (either by a human or automatically) username : username of the human annotator (if any) process_after : the field is used to add a delay between insight generation and processing, for insights that are automatically processable (to avoid product overwrite by third-party apps). reserved_barcode : if True , the product has a reserved barcode, it's probably a variable weight product. We don't show any question about reserved barcode products by default in all /questions/* API routes. Import mechanism Once the predictions are generated, we use the import_insights function (in robotoff.insights.importer ) to import the predictions and insights in database. We start by importing predictions in database, in the prediction table: we check that predictions are valid, i.e. that the product still has the image associated with the prediction (in case source_image is not null) and that the product still exists in Product Opener database. This check is disabled if ENABLE_MONGODB_ACCESS=0 (=default settings only for local environment). We then group predictions by product barcode, and delete all predictions that have the same barcode , server_type , type , source_image and that have a different predictor_version . We import predictions, only if no other predictions with the same (type, server_type, source_image, value_tag, value, predictor and automatic_processing) values exist. predictor_version is therefore used to control when to keep the previous predictions and when to delete them. From predictions, we then generate and import insights. Insights objects are created on-the-fly in InsightImporter s. Every insight type has a single InsightImporter that is in charge of creating/updating/deleting insights from a list of predictions. The importer must have a generate_candidates method that returns a list of candidate insights \u2014 ProductInsight s should be created from predictions in this method. This is also where all the selection logic is (what prediction data to keep, what to ignore). From the list of candidate insight, we update the product_insight table, by deleting all insights and importing all candidates. The import mechanism is actually a bit smarter, we avoid unnecessary DB insertion/deletion by trying to match each candidate with a reference insight \u2014 an insight that is already in DB. Once insights are refreshed/imported, they become available as questions in /questions/* API routes. Automatically processable insights are applied by the scheduler, once current_timestamp >= ${process_after} .","title":"Predictions and insights"},{"location":"explanations/predictions/#predictions-and-insights","text":"Robotoff purpose is to generate predictions about Open Food Facts products from various sources: images, image OCRs, product meta data,... A complete list of predictions types can be found in robotoff.prediction.types . The most common ones are brand , label , category ,... All predictions are stored in the PostgreSQL database in the prediction table. Predictions most interesting fields are the following: barcode : barcode of the product (string) type : prediction type value : the predicted untaxonomized value (ex: carrefour for brand prediction type), optional value_tag : the predicted taxonomized value (ex: en:organic for label prediction type), optional source_image : the path of the image the prediction was generated from (ex: /847/000/700/5117/1.jpg ). May be null, it is mainly provided for OCR and object detection-based predictions. automatic_processing : a boolean indicating whether we're confident enough in the prediction to apply it automatically in Open Food Facts without human supervision. This does not mean it will indeed be applied automatically, please refer to the import mechanism description below to know how automatic processing works. data : a JSON structure containing prediction data. It either complements value and value_tag with additional data or contains the full prediction data. predictor : name of the predictor that generated the prediction. Every insight type has its own predictor s, but most common ones are: universal-logo-detector for predictions generated by the nearest-neighbors logo detector flashtext for all predictions generated using flashtext library regex for all predictions generated using simple regex predictor_version : this is a version ID that is used to know when to replace predictions in database by new ones during import, and when to keep them. It is either an incrementing integer (for regex-based predictions) or the version of the model that generated the predictions. From these predictions, we generate insights . Insights are refined predictions about the product that are directly actionable: if the insight is validated by a human, we can update the product accordingly. For example, we may have the following predictions on a (french) organic product: prediction 1: type : label value_tag : en:organic prediction 2: type : label value_tag : fr:ab-agriculture-biologique As fr:ab-agriculture-biologique is a children of en:organic in the label taxonomy, we only generate a label insight with value_tag=fr:ab-agriculture-biologique . Furthermore, we don't generate any insight if the product already has the fr:ab-agriculture-biologique label. This is the different between insights and predictions : predictions can be viewed as raw data, and insights as actionable data. Most insight types are generated directly from their corresponding prediction types (ex: brand , label ,...). However, an insight type can require prediction of several types: this allow the generation of complex insight types. Insights can either be applied automatically or require a human annotation. Insights are saved in DB in the product_insight table. Insight fields are a superset of prediction fields \u2014 the most interesting additional fields are: annotation : either 0 (incorrect insight), 1 (correct), or -1 (invalid). Automatically applied insights have annotation=1 automatic_processing : if True, the insight will be applied automatically completed_at : timestamp of annotation (either by a human or automatically) username : username of the human annotator (if any) process_after : the field is used to add a delay between insight generation and processing, for insights that are automatically processable (to avoid product overwrite by third-party apps). reserved_barcode : if True , the product has a reserved barcode, it's probably a variable weight product. We don't show any question about reserved barcode products by default in all /questions/* API routes.","title":"Predictions and insights"},{"location":"explanations/predictions/#import-mechanism","text":"Once the predictions are generated, we use the import_insights function (in robotoff.insights.importer ) to import the predictions and insights in database. We start by importing predictions in database, in the prediction table: we check that predictions are valid, i.e. that the product still has the image associated with the prediction (in case source_image is not null) and that the product still exists in Product Opener database. This check is disabled if ENABLE_MONGODB_ACCESS=0 (=default settings only for local environment). We then group predictions by product barcode, and delete all predictions that have the same barcode , server_type , type , source_image and that have a different predictor_version . We import predictions, only if no other predictions with the same (type, server_type, source_image, value_tag, value, predictor and automatic_processing) values exist. predictor_version is therefore used to control when to keep the previous predictions and when to delete them. From predictions, we then generate and import insights. Insights objects are created on-the-fly in InsightImporter s. Every insight type has a single InsightImporter that is in charge of creating/updating/deleting insights from a list of predictions. The importer must have a generate_candidates method that returns a list of candidate insights \u2014 ProductInsight s should be created from predictions in this method. This is also where all the selection logic is (what prediction data to keep, what to ignore). From the list of candidate insight, we update the product_insight table, by deleting all insights and importing all candidates. The import mechanism is actually a bit smarter, we avoid unnecessary DB insertion/deletion by trying to match each candidate with a reference insight \u2014 an insight that is already in DB. Once insights are refreshed/imported, they become available as questions in /questions/* API routes. Automatically processable insights are applied by the scheduler, once current_timestamp >= ${process_after} .","title":"Import mechanism"},{"location":"explanations/questions/","text":"Question format proposal After loading a product, the client (web, iOS, Android) will request the Robotoff server a question (on /api/v1/questions/{barcode} ). 1 A question includes an insight and metadata such that the client knows how to display the question and what kind of input is expected from the user. All data returned by Robotoff is localized with respect to the lang parameter provided by the client. Question formats For all formats, a I don't know button will offer the user the possibility to leave without answering the question. Addition: Binary choice ( add-binary ) Add a fact about a product, by accepting (1) or rejecting (0) the insight. For instance: add a new label ( en:organic ) add a new packager code ( EMB 52052B ) Add a new category ( en:pastas ) Format type (str, required) - The question type ( add-binary ) question (str, required) - The question, in the user locale value (str, optional) - The suggested value for the field image_url (str, optional) - An image to display insight_id (str, required) - ID of the insight value or image_url cannot be both missing. Examples { \"type\" : \"add-binary\" , \"question\" : \"Does the product belong to this category ?\" , \"value\" : \"Pastas\" , \"insight_id\" : \"{INSIGHT_ID}\" , \"insight_type\" : \"category\" , \"barcode\" : \"{BARCODE}\" } { \"type\" : \"add-binary\" , \"question\" : \"Does the product have this label?\" , \"value\" : \"EU Organic\" , \"ref_image_url\" : \"https://static.openfoodfacts.org/images/lang/fr/labels/bio-europeen.135x90.png\" , \"source_image_url\" : \"https://static.openfoodfacts.org/images/products/542/503/557/7122/1.jpg\" , \"insight_id\" : \"{INSIGHT_ID}\" , \"insight_type\" : \"label\" , \"barcode\" : \"{BARCODE}\" } The ref_image_url can be used to display an image in the question interface, such as labels (IGP, organic,...). The source_image_url is the URL of the image from which the insight was extracted, if any. The client returns the insight ID and the annotation (0, -1 or 1). 2 see robotoff.app.api.ProductQuestionsResource \u21a9 see robotoff.app.api.AnnotateInsightResource \u21a9","title":"Question format proposal"},{"location":"explanations/questions/#question-format-proposal","text":"After loading a product, the client (web, iOS, Android) will request the Robotoff server a question (on /api/v1/questions/{barcode} ). 1 A question includes an insight and metadata such that the client knows how to display the question and what kind of input is expected from the user. All data returned by Robotoff is localized with respect to the lang parameter provided by the client.","title":"Question format proposal"},{"location":"explanations/questions/#question-formats","text":"For all formats, a I don't know button will offer the user the possibility to leave without answering the question.","title":"Question formats"},{"location":"explanations/questions/#addition-binary-choice-add-binary","text":"Add a fact about a product, by accepting (1) or rejecting (0) the insight. For instance: add a new label ( en:organic ) add a new packager code ( EMB 52052B ) Add a new category ( en:pastas )","title":"Addition: Binary choice (add-binary)"},{"location":"explanations/questions/#format","text":"type (str, required) - The question type ( add-binary ) question (str, required) - The question, in the user locale value (str, optional) - The suggested value for the field image_url (str, optional) - An image to display insight_id (str, required) - ID of the insight value or image_url cannot be both missing.","title":"Format"},{"location":"explanations/questions/#examples","text":"{ \"type\" : \"add-binary\" , \"question\" : \"Does the product belong to this category ?\" , \"value\" : \"Pastas\" , \"insight_id\" : \"{INSIGHT_ID}\" , \"insight_type\" : \"category\" , \"barcode\" : \"{BARCODE}\" } { \"type\" : \"add-binary\" , \"question\" : \"Does the product have this label?\" , \"value\" : \"EU Organic\" , \"ref_image_url\" : \"https://static.openfoodfacts.org/images/lang/fr/labels/bio-europeen.135x90.png\" , \"source_image_url\" : \"https://static.openfoodfacts.org/images/products/542/503/557/7122/1.jpg\" , \"insight_id\" : \"{INSIGHT_ID}\" , \"insight_type\" : \"label\" , \"barcode\" : \"{BARCODE}\" } The ref_image_url can be used to display an image in the question interface, such as labels (IGP, organic,...). The source_image_url is the URL of the image from which the insight was extracted, if any. The client returns the insight ID and the annotation (0, -1 or 1). 2 see robotoff.app.api.ProductQuestionsResource \u21a9 see robotoff.app.api.AnnotateInsightResource \u21a9","title":"Examples"},{"location":"explanations/triton/","text":"Triton inference server We use the Triton Inference Server to serve ML models. A CLI is available to easily manage models (add new model, load/unload, enable/disable). For further information on how Triton manages model we refer the reader to the last section . Triton deployment Triton can be run directly on the same server as Robotoff, but in production it runs on a distinct server with GPUs (server gpu-01 , deployed on Google Cloud). To deploy Triton on a distinct server, go to models , where you will find a docker-compose.yml file to run the Triton server in a Docker container: cd models docker-compose up -d For local development, you can also run Triton on your local machine with Docker. Simply make sure that you docker/ml.yml is part of your COMPOSE_FILE envvar in your .env (ex: COMPOSE_FILE=docker-compose.yml;docker/dev.yml;docker/ml.yml ). Triton model management A script manage.py can be found in the models directory to help manage models on the Triton server. It can be used to download, list, load, and unload models. The easiest way to launch the script is using uv . Downloading models To download all models, run: uv run manage.py download-models It downloads all models defined in the models/models.toml file and store them in the models/triton directory. If you need to download a specific model (and ignore the remaining models defined in the toml file), you can provide its name with the --include option: uv run manage.py download-models --include MODEL_NAME Copying configuration files Triton can generate automatically a configuration file for ONNX model, but we have custom configuration files as it allows us to have more control over the model behavior. Configuration files are stored in the models/triton-config directory. To copy them to the models/triton directory, run: uv run manage.py copy-config By default, it copies the GPU configuration files. If you're running on CPU, use the --cpu option. Once the configuration files are copied, you can restart the Triton server: docker compose restart triton Make sure to check the logs to ensure that the models are loaded correctly: docker compose logs -f triton Listing models To list the models currently loaded on the Triton server, run: uv run manage.py list-models Make sure that the Triton server is running before executing this command. Loading and unloading models All models present in the models/triton directory are loaded at Triton startup. If you add a new model, you don't need to restart the server, you can load it dynamically. Use the following command to load a model: uv run manage.py load-model --model MODEL_NAME You can unload a model with the following command: uv run manage.py unload-model --model MODEL_NAME Disabling models When you work locally, you don't always want to load all models, as it can take a lot of RAM. You can disable a model by using the following command: uv run manage.py disable-model --model MODEL_NAME This command moves the model directory from models/triton to models/triton-disabled . You can re-enable a model by moving it back: uv run manage.py enable-model --model MODEL_NAME Get a model configuration from Triton You can get the configuration of a model currently loaded in Triton with the following command: uv run manage.py get-model-config --model MODEL_NAME This command retrieves the model configuration from the Triton server and displays it in the console. Reference on Triton model management & configuration Models are exported in ONNX format and added in the models/triton directory. Triton expects the models to be in a specific directory structure, with the model files in a subdirectory named after the model. For example, the model my_model should be in a directory named my_model in the models/triton directory. Each model version should be in a subdirectory named after the version number, for example 1 . Triton offers APIs in HTTP and gRPC. Robotoff uses the gRPC API to communicate with the server. Triton possesses several model management mode, we use the 'explicit' mode. In this mode, the server does not load any model by default, and models must be explicitly loaded and unloaded by the client. We ask Triton to load all the models in the models/triton directory at startup. Using this mode, we don't have to restart the server when we add or remove models.","title":"Triton inference server"},{"location":"explanations/triton/#triton-inference-server","text":"We use the Triton Inference Server to serve ML models. A CLI is available to easily manage models (add new model, load/unload, enable/disable). For further information on how Triton manages model we refer the reader to the last section .","title":"Triton inference server"},{"location":"explanations/triton/#triton-deployment","text":"Triton can be run directly on the same server as Robotoff, but in production it runs on a distinct server with GPUs (server gpu-01 , deployed on Google Cloud). To deploy Triton on a distinct server, go to models , where you will find a docker-compose.yml file to run the Triton server in a Docker container: cd models docker-compose up -d For local development, you can also run Triton on your local machine with Docker. Simply make sure that you docker/ml.yml is part of your COMPOSE_FILE envvar in your .env (ex: COMPOSE_FILE=docker-compose.yml;docker/dev.yml;docker/ml.yml ).","title":"Triton deployment"},{"location":"explanations/triton/#triton-model-management","text":"A script manage.py can be found in the models directory to help manage models on the Triton server. It can be used to download, list, load, and unload models. The easiest way to launch the script is using uv .","title":"Triton model management"},{"location":"explanations/triton/#downloading-models","text":"To download all models, run: uv run manage.py download-models It downloads all models defined in the models/models.toml file and store them in the models/triton directory. If you need to download a specific model (and ignore the remaining models defined in the toml file), you can provide its name with the --include option: uv run manage.py download-models --include MODEL_NAME","title":"Downloading models"},{"location":"explanations/triton/#copying-configuration-files","text":"Triton can generate automatically a configuration file for ONNX model, but we have custom configuration files as it allows us to have more control over the model behavior. Configuration files are stored in the models/triton-config directory. To copy them to the models/triton directory, run: uv run manage.py copy-config By default, it copies the GPU configuration files. If you're running on CPU, use the --cpu option. Once the configuration files are copied, you can restart the Triton server: docker compose restart triton Make sure to check the logs to ensure that the models are loaded correctly: docker compose logs -f triton","title":"Copying configuration files"},{"location":"explanations/triton/#listing-models","text":"To list the models currently loaded on the Triton server, run: uv run manage.py list-models Make sure that the Triton server is running before executing this command.","title":"Listing models"},{"location":"explanations/triton/#loading-and-unloading-models","text":"All models present in the models/triton directory are loaded at Triton startup. If you add a new model, you don't need to restart the server, you can load it dynamically. Use the following command to load a model: uv run manage.py load-model --model MODEL_NAME You can unload a model with the following command: uv run manage.py unload-model --model MODEL_NAME","title":"Loading and unloading models"},{"location":"explanations/triton/#disabling-models","text":"When you work locally, you don't always want to load all models, as it can take a lot of RAM. You can disable a model by using the following command: uv run manage.py disable-model --model MODEL_NAME This command moves the model directory from models/triton to models/triton-disabled . You can re-enable a model by moving it back: uv run manage.py enable-model --model MODEL_NAME","title":"Disabling models"},{"location":"explanations/triton/#get-a-model-configuration-from-triton","text":"You can get the configuration of a model currently loaded in Triton with the following command: uv run manage.py get-model-config --model MODEL_NAME This command retrieves the model configuration from the Triton server and displays it in the console.","title":"Get a model configuration from Triton"},{"location":"explanations/triton/#reference-on-triton-model-management-configuration","text":"Models are exported in ONNX format and added in the models/triton directory. Triton expects the models to be in a specific directory structure, with the model files in a subdirectory named after the model. For example, the model my_model should be in a directory named my_model in the models/triton directory. Each model version should be in a subdirectory named after the version number, for example 1 . Triton offers APIs in HTTP and gRPC. Robotoff uses the gRPC API to communicate with the server. Triton possesses several model management mode, we use the 'explicit' mode. In this mode, the server does not load any model by default, and models must be explicitly loaded and unloaded by the client. We ask Triton to load all the models in the models/triton directory at startup. Using this mode, we don't have to restart the server when we add or remove models.","title":"Reference on Triton model management &amp; configuration"},{"location":"how-to-guides/add-predictor/","text":"How to add a new model/predictor to Robotoff? You have a shiny new model and you want to add it to Robotoff? Great! Here is a step-by-step guide to help you. Prediction generation All prediction-related code lives in robotoff.prediction module. If the prediction is OCR-based, it should be in robotoff.prediction.ocr , otherwise at the root of the module (you can create a deeper module if necessary). If you want to deploy an ML model, the best place to serve it is on Triton, our ML inference server. You should first convert your model to SavedModel or ONNX format and make it servable through Triton. Inference requests to Triton are sent through gRPC 1 . The result of the computation should be a list of Prediction s. If you want the insight to be automatically applied, set automatic_processing=True when creating the Prediction. You may need to create a new entry in PredictionType and InsightType in robotoff.types depending on your predictor. Calling the predictor If the prediction is not OCR-based, you will either want the predictor to be called: when a new image is uploaded: you should add a call to your predictor function in robotoff.workers.tasks.import_image when the product is updated: you should add a call to your predictor function in robotoff.workers.tasks.product_updated Importing predictions/insights The next step is to import the predictions in database and generate insights from these predictions. If you don't know the difference between insights and predictions, check this page . You should create a new importer class subclassing InsightImporter in robotoff.insights.importer and add it to the IMPORTERS list. Trigger actions after insight annotation To perform some actions when the insight has been annotated and marked as correct ( annotation=1 ), you should add a new class subclassing InsightAnnotator in robotoff.insights.annotate and add it to the ANNOTATOR_MAPPING dictionary. If you need a call to Product Opener API that is not implemented yet, add it to robotoff.off . Test your predictor To test your predictor, you can simulate an image import by adding an event to the Redis Stream. There is a CLI command that does this for you: make robotoff-cli args = 'create-redis-update --barcode 3770016266048 --uploaded-image-id 1' This command will create an event in the product_updates_off stream with the barcode 3770016266048 , action updated and diffs {\"uploaded_images\": {\"add\": [\"1\"]}} . You can check that: the predictor is indeed called it generates predictions/insights and save them in DB when expected no errors occurred (in the logs) If you add an OCR-only predictor, you should also add unit tests in tests/unit . We don't yet have the possibility to perform integration tests on ML models served by Triton though. That's it, congratulations \ud83c\udf89! see generate_clip_embedding in robotoff.triton for an example \u21a9","title":"How to add a new model/predictor to Robotoff?"},{"location":"how-to-guides/add-predictor/#how-to-add-a-new-modelpredictor-to-robotoff","text":"You have a shiny new model and you want to add it to Robotoff? Great! Here is a step-by-step guide to help you.","title":"How to add a new model/predictor to Robotoff?"},{"location":"how-to-guides/add-predictor/#prediction-generation","text":"All prediction-related code lives in robotoff.prediction module. If the prediction is OCR-based, it should be in robotoff.prediction.ocr , otherwise at the root of the module (you can create a deeper module if necessary). If you want to deploy an ML model, the best place to serve it is on Triton, our ML inference server. You should first convert your model to SavedModel or ONNX format and make it servable through Triton. Inference requests to Triton are sent through gRPC 1 . The result of the computation should be a list of Prediction s. If you want the insight to be automatically applied, set automatic_processing=True when creating the Prediction. You may need to create a new entry in PredictionType and InsightType in robotoff.types depending on your predictor.","title":"Prediction generation"},{"location":"how-to-guides/add-predictor/#calling-the-predictor","text":"If the prediction is not OCR-based, you will either want the predictor to be called: when a new image is uploaded: you should add a call to your predictor function in robotoff.workers.tasks.import_image when the product is updated: you should add a call to your predictor function in robotoff.workers.tasks.product_updated","title":"Calling the predictor"},{"location":"how-to-guides/add-predictor/#importing-predictionsinsights","text":"The next step is to import the predictions in database and generate insights from these predictions. If you don't know the difference between insights and predictions, check this page . You should create a new importer class subclassing InsightImporter in robotoff.insights.importer and add it to the IMPORTERS list.","title":"Importing predictions/insights"},{"location":"how-to-guides/add-predictor/#trigger-actions-after-insight-annotation","text":"To perform some actions when the insight has been annotated and marked as correct ( annotation=1 ), you should add a new class subclassing InsightAnnotator in robotoff.insights.annotate and add it to the ANNOTATOR_MAPPING dictionary. If you need a call to Product Opener API that is not implemented yet, add it to robotoff.off .","title":"Trigger actions after insight annotation"},{"location":"how-to-guides/add-predictor/#test-your-predictor","text":"To test your predictor, you can simulate an image import by adding an event to the Redis Stream. There is a CLI command that does this for you: make robotoff-cli args = 'create-redis-update --barcode 3770016266048 --uploaded-image-id 1' This command will create an event in the product_updates_off stream with the barcode 3770016266048 , action updated and diffs {\"uploaded_images\": {\"add\": [\"1\"]}} . You can check that: the predictor is indeed called it generates predictions/insights and save them in DB when expected no errors occurred (in the logs) If you add an OCR-only predictor, you should also add unit tests in tests/unit . We don't yet have the possibility to perform integration tests on ML models served by Triton though. That's it, congratulations \ud83c\udf89! see generate_clip_embedding in robotoff.triton for an example \u21a9","title":"Test your predictor"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/","text":"Implementing Robotoff Questions in 3rd Party Apps This tutorial explains how to implement Robotoff questions in your 3rd party application, allowing users to contribute to food transparency and data accuracy on Open Food Facts. Rationale Robotoff questions help increase food transparency and simplify obtaining the Nutri-Score by enabling categorization of products (and more). How to Implement API Reference For detailed API endpoints and their specifications, refer to the official Robotoff API documentation: https://openfoodfacts.github.io/robotoff/references/api/#tag/Question-Management Displaying Questions * Visibility for Non-Logged Users: Questions should be visible to users even if they are not logged in. * Prompt for Non-Logged Users: If a non-logged user attempts to answer a question, display a prompt to log in or create an account (or let them use our anonymous statistical voting system). Provide a setting to hide these prompts if they annoy the user. * Fetching Questions: When a product is opened in your application, fetch the relevant Robotoff questions. * Example API Call: https://robotoff.openfoodfacts.org/api/v1/questions/3274570800026?lang=en&count=3 This example fetches 3 questions for the barcode 3274570800026 in English. * Response Structure Example: { \"questions\" : [ { \"barcode\" : \"3274570800026\" , \"type\" : \"add-binary\" , \"value\" : \"Scallop\" , \"question\" : \"Does the product belong to this category?\" , \"insight_id\" : \"5cac03bc-a5a7-4ec2-a548-17fd9319fee7\" , \"insight_type\" : \"category\" , \"source_image_url\" : \"https://static.openfoodfacts.org/images/products/327/457/080/0026/front_en.4.400.jpg\" } ], \"status\" : \"found\" } * The lang field in the request specifies the language of the returned question and value . * UI Display: Display the question and possible answers in your application's user interface. Sending Answers If a user answers a question, send the appropriate ping back to the Open Food Facts server. * API for Annotating Insights: https://robotoff.openfoodfacts.org/api/v1/insights/annotate?insight_id=(insight_id)&annotation=(1,0,-1)&update=1 * Replace (insight_id) with the insight_id received from the question payload. * Replace (1,0,-1) with the user's annotation: * 1 : Yes * 0 : No * -1 : Skip/Don't know Authentication To give credit to contributors for their answers, you need to authenticate your requests to Robotoff. Header-based Authentication Send the following header with your requests: Authorization: Basic (base64Credentials) Where base64Credentials is the Base64 encoding of your username:password . Cookie-based Authentication Note: we have a cookie auth for tools hosted on *.openfoodfacts.org. Please reachout to the Robotoff team if needed. Platform-Specific Implementations Web component available at https://github.com/openfoodfacts/openfoodfacts-webcomponents Flutter/Dart (available in our Dart package, and UI code is available in the official smooth-app repository Android (old official app, Kotlin, some code might be usable) * https://github.com/openfoodfacts/openfoodfacts-androidapp/issues/3024 * https://github.com/openfoodfacts/openfoodfacts-androidapp/issues/2931 iOS (old official app, Swift, some code might be usable) * https://github.com/openfoodfacts/openfoodfacts-ios","title":"Implementing Robotoff Questions in 3rd Party Apps"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#implementing-robotoff-questions-in-3rd-party-apps","text":"This tutorial explains how to implement Robotoff questions in your 3rd party application, allowing users to contribute to food transparency and data accuracy on Open Food Facts.","title":"Implementing Robotoff Questions in 3rd Party Apps"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#rationale","text":"Robotoff questions help increase food transparency and simplify obtaining the Nutri-Score by enabling categorization of products (and more).","title":"Rationale"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#how-to-implement","text":"","title":"How to Implement"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#api-reference","text":"For detailed API endpoints and their specifications, refer to the official Robotoff API documentation: https://openfoodfacts.github.io/robotoff/references/api/#tag/Question-Management","title":"API Reference"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#displaying-questions","text":"* Visibility for Non-Logged Users: Questions should be visible to users even if they are not logged in. * Prompt for Non-Logged Users: If a non-logged user attempts to answer a question, display a prompt to log in or create an account (or let them use our anonymous statistical voting system). Provide a setting to hide these prompts if they annoy the user. * Fetching Questions: When a product is opened in your application, fetch the relevant Robotoff questions. * Example API Call: https://robotoff.openfoodfacts.org/api/v1/questions/3274570800026?lang=en&count=3 This example fetches 3 questions for the barcode 3274570800026 in English. * Response Structure Example: { \"questions\" : [ { \"barcode\" : \"3274570800026\" , \"type\" : \"add-binary\" , \"value\" : \"Scallop\" , \"question\" : \"Does the product belong to this category?\" , \"insight_id\" : \"5cac03bc-a5a7-4ec2-a548-17fd9319fee7\" , \"insight_type\" : \"category\" , \"source_image_url\" : \"https://static.openfoodfacts.org/images/products/327/457/080/0026/front_en.4.400.jpg\" } ], \"status\" : \"found\" } * The lang field in the request specifies the language of the returned question and value . * UI Display: Display the question and possible answers in your application's user interface.","title":"Displaying Questions"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#sending-answers","text":"If a user answers a question, send the appropriate ping back to the Open Food Facts server. * API for Annotating Insights: https://robotoff.openfoodfacts.org/api/v1/insights/annotate?insight_id=(insight_id)&annotation=(1,0,-1)&update=1 * Replace (insight_id) with the insight_id received from the question payload. * Replace (1,0,-1) with the user's annotation: * 1 : Yes * 0 : No * -1 : Skip/Don't know","title":"Sending Answers"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#authentication","text":"To give credit to contributors for their answers, you need to authenticate your requests to Robotoff.","title":"Authentication"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#header-based-authentication","text":"Send the following header with your requests: Authorization: Basic (base64Credentials) Where base64Credentials is the Base64 encoding of your username:password .","title":"Header-based Authentication"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#cookie-based-authentication","text":"Note: we have a cookie auth for tools hosted on *.openfoodfacts.org. Please reachout to the Robotoff team if needed.","title":"Cookie-based Authentication"},{"location":"how-to-guides/robotoff-for-3rd-party-apps/#platform-specific-implementations","text":"Web component available at https://github.com/openfoodfacts/openfoodfacts-webcomponents Flutter/Dart (available in our Dart package, and UI code is available in the official smooth-app repository Android (old official app, Kotlin, some code might be usable) * https://github.com/openfoodfacts/openfoodfacts-androidapp/issues/3024 * https://github.com/openfoodfacts/openfoodfacts-androidapp/issues/2931 iOS (old official app, Swift, some code might be usable) * https://github.com/openfoodfacts/openfoodfacts-ios","title":"Platform-Specific Implementations"},{"location":"how-to-guides/test-and-debug/","text":"Test and debug This documentation intends to: 1) Familiarize you on running our test cases. 2) Give you some tips from getting stuck if you are working only on Robotoff and don't need complete installation of Open Food Facts server. How to generate data? Robotoff is an API that pulls prediction data, annotation data, product data, nutrition data from MongoDB database in Open Food Facts server. If your development instance is not connected to a product-opener instance (which happens automatically if you have a running product-opener instance), you won't have a MongoDB instance. This means you won't have any product data on your local set up. Though you may populate your postgres database with some predictions, insights, images references, etc. We recommend Factory to create some data in your local database. If you have installed Robotoff via Docker, you can run Python using uv and execute Factory like so: $ docker compose run --rm api uv run python ... > from tests.integration.models_utils import * > PredictionFactory() NOTE: If you are on Windows we recommend using Git bash to run commands. How to run test cases? We use pytest to run test cases and Makefile to run our commands. The following command will run all the test cases one by one: $ make tests How to run a single test case? The simplest is to use the pytest make target for that: make pytest args = 'path/to/test_file.py::the_function_name' For example, to call test_get_type() from tests/unit/insights/test_importer.py : $ make pytest args = \"tests/unit/insights/test_importer.py::TestLabelInsightImporter::test_get_type\" Remember to put quotes especially if you have multiple arguments. NOTE : Be sure to run make create_external_networks before if needed (especially if you get Network po_test declared as external, but could not be found ) When to write your own test cases? Write test cases every time you write a new feature, to test a feature or to understand the working of an existing function. Automated testing really helps to prevent future bugs as we introduce new features or refactor the code. There are instances when Robotoff tries to connect to MongoDB via Open Food Facts server. To disable this feature (this is disabled by default on local environments), set ENABLE_MONGODB_ACCESS=0 in your .env . Debugging guide We encourage using PDB to debug. Running test with --pdb flags, pytest will stop and open the pdb console as soon as there is an error or an assert fails. This can be a good way to try to understand why a test is failing. make pytest args = \"path/to/test.py --pdb\" If it's a mock.assert_called_with , you can look at the real data passed to a test case by calling mock.call_args in the pdb console. If you need more precise control to see code path before it breaks, you can add the following lines in your function to find out what your code does and where it breaks. import pdb ; pdb . set_trace () and then run the pytest , with the --pdb option (as above). Note we need the --pdb option, to view the inputs and outputs captured by pytest and access the pdb console. How to run checks locally When committing your modifications to the main branch, your code have to pass several tests automatically run by Github in order to be merged. You can run theses checks locally before committing by using the following command: $ make checks","title":"Test and debug"},{"location":"how-to-guides/test-and-debug/#test-and-debug","text":"","title":"Test and debug"},{"location":"how-to-guides/test-and-debug/#this-documentation-intends-to","text":"1) Familiarize you on running our test cases. 2) Give you some tips from getting stuck if you are working only on Robotoff and don't need complete installation of Open Food Facts server.","title":"This documentation intends to:"},{"location":"how-to-guides/test-and-debug/#how-to-generate-data","text":"Robotoff is an API that pulls prediction data, annotation data, product data, nutrition data from MongoDB database in Open Food Facts server. If your development instance is not connected to a product-opener instance (which happens automatically if you have a running product-opener instance), you won't have a MongoDB instance. This means you won't have any product data on your local set up. Though you may populate your postgres database with some predictions, insights, images references, etc. We recommend Factory to create some data in your local database. If you have installed Robotoff via Docker, you can run Python using uv and execute Factory like so: $ docker compose run --rm api uv run python ... > from tests.integration.models_utils import * > PredictionFactory() NOTE: If you are on Windows we recommend using Git bash to run commands.","title":"How to generate data?"},{"location":"how-to-guides/test-and-debug/#how-to-run-test-cases","text":"We use pytest to run test cases and Makefile to run our commands. The following command will run all the test cases one by one: $ make tests","title":"How to run test cases?"},{"location":"how-to-guides/test-and-debug/#how-to-run-a-single-test-case","text":"The simplest is to use the pytest make target for that: make pytest args = 'path/to/test_file.py::the_function_name' For example, to call test_get_type() from tests/unit/insights/test_importer.py : $ make pytest args = \"tests/unit/insights/test_importer.py::TestLabelInsightImporter::test_get_type\" Remember to put quotes especially if you have multiple arguments. NOTE : Be sure to run make create_external_networks before if needed (especially if you get Network po_test declared as external, but could not be found )","title":"How to run a single test case?"},{"location":"how-to-guides/test-and-debug/#when-to-write-your-own-test-cases","text":"Write test cases every time you write a new feature, to test a feature or to understand the working of an existing function. Automated testing really helps to prevent future bugs as we introduce new features or refactor the code. There are instances when Robotoff tries to connect to MongoDB via Open Food Facts server. To disable this feature (this is disabled by default on local environments), set ENABLE_MONGODB_ACCESS=0 in your .env .","title":"When to write your own test cases?"},{"location":"how-to-guides/test-and-debug/#debugging-guide","text":"We encourage using PDB to debug. Running test with --pdb flags, pytest will stop and open the pdb console as soon as there is an error or an assert fails. This can be a good way to try to understand why a test is failing. make pytest args = \"path/to/test.py --pdb\" If it's a mock.assert_called_with , you can look at the real data passed to a test case by calling mock.call_args in the pdb console. If you need more precise control to see code path before it breaks, you can add the following lines in your function to find out what your code does and where it breaks. import pdb ; pdb . set_trace () and then run the pytest , with the --pdb option (as above). Note we need the --pdb option, to view the inputs and outputs captured by pytest and access the pdb console.","title":"Debugging guide"},{"location":"how-to-guides/test-and-debug/#how-to-run-checks-locally","text":"When committing your modifications to the main branch, your code have to pass several tests automatically run by Github in order to be merged. You can run theses checks locally before committing by using the following command: $ make checks","title":"How to run checks locally"},{"location":"how-to-guides/deployment/dev-install/","text":"Dev install You may choose Docker install (recommended, less chance to mess up with your system, all included) or local install. Docker install After cloning the repository, customize parameters by editing the .env file. You should, consider those changes: if you want to use ML models, add docker/ml.yml to COMPOSE_FILE , using ; as separator. You should also download all models by running the following command inside the models directory: uv run manage.py download-models . See this documentation page for more information about model management. change OFF_UID and OFF_GID to match your own user UID/GID (optional, only if you experience some file permission issue, see Getting developper uid for docker ) Note: beware not to commit your local changes to .env file! Because of Elasticsearch service, you may need to increase a system parameter ( vm.max_map_count=262144 ), as described here . Then simply run: make dev This will build containers, pull images based containers, create containers and run them. Verify whether robotoff is running as expected, by executing the following command in CLI: curl http://localhost:5500/api/v1/status The expected response is {\"status\":\"running\"} . Also take a look at maintenance . To debug in a running container, you can simply run: docker compose run --rm api python Here we run the api service. This opens a Python command prompt, you may debug with pdb or play with the code. Local install with uv This is an alternative if you are reluctant to use Docker, or have some other reasons to prefer manual install. After cloning the repository: 1. Make sure a recent version of uv is installed Install the dependencies: uv sync Configure files required for the tests to run locally: Compile the i18n files: cd i18n && bash compile.sh && cd .. Also configure your settings to point to your dev PostgreSQL database (that you should have installed the way you want) Restore DB dumps To have real-world data, you're probably interested in restoring data from production server. PostgreSQL dump Robotoff uses PostgreSQL as main database. First, download the latest DB dump: wget https://openfoodfacts.org/data/dumps/robotoff_postgres_latest.dump Start PostgreSQL container and copy the dump inside the container: make up service = postgres docker cp -a robotoff_postgres_latest.dump robotoff-postgres-1:/tmp/ Then launch dump restore: docker exec -it robotoff-postgres-1 pg_restore -v -d postgres -U postgres -c -j 8 --if-exists /tmp/robotoff_postgres_latest.dump This command drops all existing tables ( -c command) and perform restore using 8 cores. The database is huge, it may take several hours to run depending on your hardware. MongoDB dump Robotoff also relies on MongoDB to fetch product. On staging and production, it interacts directly with the same MongoDB instance used by Product Opener. To restore Product Opener MongoDB dump, start by downloading and extracting the archive: wget https://static.openfoodfacts.org/data/openfoodfacts-mongodbdump.tar.gz tar xvzf openfoodfacts-mongodbdump.tar.gz Make sure the MongoDB container is up and running and copy the dump directory inside the container: make up service = mongodb docker cp -a dump robotoff_mongodb_1:/var/tmp/ Then launch dump restore: docker exec -it robotoff_mongodb_1 mongorestore --drop /var/tmp/dump","title":"Dev install"},{"location":"how-to-guides/deployment/dev-install/#dev-install","text":"You may choose Docker install (recommended, less chance to mess up with your system, all included) or local install.","title":"Dev install"},{"location":"how-to-guides/deployment/dev-install/#docker-install","text":"After cloning the repository, customize parameters by editing the .env file. You should, consider those changes: if you want to use ML models, add docker/ml.yml to COMPOSE_FILE , using ; as separator. You should also download all models by running the following command inside the models directory: uv run manage.py download-models . See this documentation page for more information about model management. change OFF_UID and OFF_GID to match your own user UID/GID (optional, only if you experience some file permission issue, see Getting developper uid for docker ) Note: beware not to commit your local changes to .env file! Because of Elasticsearch service, you may need to increase a system parameter ( vm.max_map_count=262144 ), as described here . Then simply run: make dev This will build containers, pull images based containers, create containers and run them. Verify whether robotoff is running as expected, by executing the following command in CLI: curl http://localhost:5500/api/v1/status The expected response is {\"status\":\"running\"} . Also take a look at maintenance . To debug in a running container, you can simply run: docker compose run --rm api python Here we run the api service. This opens a Python command prompt, you may debug with pdb or play with the code.","title":"Docker install"},{"location":"how-to-guides/deployment/dev-install/#local-install-with-uv","text":"This is an alternative if you are reluctant to use Docker, or have some other reasons to prefer manual install. After cloning the repository: 1. Make sure a recent version of uv is installed Install the dependencies: uv sync Configure files required for the tests to run locally: Compile the i18n files: cd i18n && bash compile.sh && cd .. Also configure your settings to point to your dev PostgreSQL database (that you should have installed the way you want)","title":"Local install with uv"},{"location":"how-to-guides/deployment/dev-install/#restore-db-dumps","text":"To have real-world data, you're probably interested in restoring data from production server.","title":"Restore DB dumps"},{"location":"how-to-guides/deployment/dev-install/#postgresql-dump","text":"Robotoff uses PostgreSQL as main database. First, download the latest DB dump: wget https://openfoodfacts.org/data/dumps/robotoff_postgres_latest.dump Start PostgreSQL container and copy the dump inside the container: make up service = postgres docker cp -a robotoff_postgres_latest.dump robotoff-postgres-1:/tmp/ Then launch dump restore: docker exec -it robotoff-postgres-1 pg_restore -v -d postgres -U postgres -c -j 8 --if-exists /tmp/robotoff_postgres_latest.dump This command drops all existing tables ( -c command) and perform restore using 8 cores. The database is huge, it may take several hours to run depending on your hardware.","title":"PostgreSQL dump"},{"location":"how-to-guides/deployment/dev-install/#mongodb-dump","text":"Robotoff also relies on MongoDB to fetch product. On staging and production, it interacts directly with the same MongoDB instance used by Product Opener. To restore Product Opener MongoDB dump, start by downloading and extracting the archive: wget https://static.openfoodfacts.org/data/openfoodfacts-mongodbdump.tar.gz tar xvzf openfoodfacts-mongodbdump.tar.gz Make sure the MongoDB container is up and running and copy the dump directory inside the container: make up service = mongodb docker cp -a dump robotoff_mongodb_1:/var/tmp/ Then launch dump restore: docker exec -it robotoff_mongodb_1 mongorestore --drop /var/tmp/dump","title":"MongoDB dump"},{"location":"how-to-guides/deployment/maintenance/","text":"Services maintenance Robotoff is split in several services: the scheduler , responsible for launching recurrent tasks (downloading new dataset, processing insights automatically,...) the workers , responsible for all long-lasting tasks (mainly insight extraction from images) the public api service the triton service which serve ML models Two additional services are used: a PostgreSQL database ( postgres service) a Elasticsearch single node ( elasticsearch service) All services are managed by docker. docker-compose is used to manage these services. tf-serving and triton have their own file: docker/ml.yml . ML models are stored on the Hugging Face Hub . All Robotoff services are running on one of the two Docker instances (OVH 200 VM for staging and OVH 201 VM for production). You should use the proxy servers ( ovh1.openfoodfacts.org or ovh2.openfoofacts.org ) to reach these instances. You can get more information on Docker VMs here . Quick start see dev-install You can then use: docker compose start [service-name] or docker compose stop [service-name] Or make up when you refresh the product (it will re-build and run docker compose up -d ). Take the time to become a bit familiar with docker-compose if it's your first use. Monitor See logs To display the logs of the container, docker compose logs [service-name] . (without service-name, you got all logs). Two options are often used: -f to follow output and --tail n to only display last n lines. To display all running services, run make status : Name Command State Ports ---------------------------------------------------------------------------------------------------- robotoff-api-1 /bin/sh -c /docker-entrypo ... Up 0.0.0.0:5500->5500/tcp,:::5500->5500 /tcp robotoff-postgres-1 docker-entrypoint.sh postg ... Up 127.0.0.1:5432->5432/tcp robotoff-scheduler-1 /bin/sh -c /docker-entrypo ... Up robotoff-worker-1 /bin/sh -c /docker-entrypo ... Up robotoff-worker-2 /bin/sh -c /docker-entrypo ... Up ... See number of tasks in queues If you want to monitor how much job robotoff has to do (how behind it is), you can run the rq command to get status: docker compose run --rm --no-deps worker-1 rq info This may help you understand why robotoff insight are not visible immediately on products. See also rq monitoring documentation for more commands and informations. Database backup and restore To backup the PostgreSQL database, run the following command: docker exec -i robotoff_postgres_1 pg_dump --schema public -F c -U postgres postgres | gzip > robotoff_postgres_ $( date +%Y-%m-%d ) .dump All Robotoff PostgreSQL dumps are stored on openfoodfacts.org server, in /srv2/off/html/data/dumps folder. When backing up the database, please update the robotoff_postgres_latest.dump symlink so that http://openfoodfacts.org/data/dumps/robotoff_postgres_latest.dump always points to the latest dump. You can restore it easily by copying the dump file inside the container and launching pg_restore : docker cp -a robotoff_postgres.dump robotoff_postgres_1:/tmp/ docker exec -it robotoff_postgres_1 pg_restore -v -d postgres -U postgres -j 8 --if-exists /tmp/robotoff_postgres.dump","title":"Services maintenance"},{"location":"how-to-guides/deployment/maintenance/#services-maintenance","text":"Robotoff is split in several services: the scheduler , responsible for launching recurrent tasks (downloading new dataset, processing insights automatically,...) the workers , responsible for all long-lasting tasks (mainly insight extraction from images) the public api service the triton service which serve ML models Two additional services are used: a PostgreSQL database ( postgres service) a Elasticsearch single node ( elasticsearch service) All services are managed by docker. docker-compose is used to manage these services. tf-serving and triton have their own file: docker/ml.yml . ML models are stored on the Hugging Face Hub . All Robotoff services are running on one of the two Docker instances (OVH 200 VM for staging and OVH 201 VM for production). You should use the proxy servers ( ovh1.openfoodfacts.org or ovh2.openfoofacts.org ) to reach these instances. You can get more information on Docker VMs here .","title":"Services maintenance"},{"location":"how-to-guides/deployment/maintenance/#quick-start","text":"see dev-install You can then use: docker compose start [service-name] or docker compose stop [service-name] Or make up when you refresh the product (it will re-build and run docker compose up -d ). Take the time to become a bit familiar with docker-compose if it's your first use.","title":"Quick start"},{"location":"how-to-guides/deployment/maintenance/#monitor","text":"","title":"Monitor"},{"location":"how-to-guides/deployment/maintenance/#see-logs","text":"To display the logs of the container, docker compose logs [service-name] . (without service-name, you got all logs). Two options are often used: -f to follow output and --tail n to only display last n lines. To display all running services, run make status : Name Command State Ports ---------------------------------------------------------------------------------------------------- robotoff-api-1 /bin/sh -c /docker-entrypo ... Up 0.0.0.0:5500->5500/tcp,:::5500->5500 /tcp robotoff-postgres-1 docker-entrypoint.sh postg ... Up 127.0.0.1:5432->5432/tcp robotoff-scheduler-1 /bin/sh -c /docker-entrypo ... Up robotoff-worker-1 /bin/sh -c /docker-entrypo ... Up robotoff-worker-2 /bin/sh -c /docker-entrypo ... Up ...","title":"See logs"},{"location":"how-to-guides/deployment/maintenance/#see-number-of-tasks-in-queues","text":"If you want to monitor how much job robotoff has to do (how behind it is), you can run the rq command to get status: docker compose run --rm --no-deps worker-1 rq info This may help you understand why robotoff insight are not visible immediately on products. See also rq monitoring documentation for more commands and informations.","title":"See number of tasks in queues"},{"location":"how-to-guides/deployment/maintenance/#database-backup-and-restore","text":"To backup the PostgreSQL database, run the following command: docker exec -i robotoff_postgres_1 pg_dump --schema public -F c -U postgres postgres | gzip > robotoff_postgres_ $( date +%Y-%m-%d ) .dump All Robotoff PostgreSQL dumps are stored on openfoodfacts.org server, in /srv2/off/html/data/dumps folder. When backing up the database, please update the robotoff_postgres_latest.dump symlink so that http://openfoodfacts.org/data/dumps/robotoff_postgres_latest.dump always points to the latest dump. You can restore it easily by copying the dump file inside the container and launching pg_restore : docker cp -a robotoff_postgres.dump robotoff_postgres_1:/tmp/ docker exec -it robotoff_postgres_1 pg_restore -v -d postgres -U postgres -j 8 --if-exists /tmp/robotoff_postgres.dump","title":"Database backup and restore"},{"location":"introduction/architecture/","text":"Architecture Robotoff is made of several services: the public API service the scheduler , responsible for launching recurrent tasks (downloading new dataset, processing insights automatically,...) 1 the workers , responsible for all long-lasting tasks a redis instance specific to Robotoff, used to handle locks and messaging queues the update listener , responsible for listening to Product Opener events and triggering actions accordingly a PostgreSQL database, where all Robotoff data are stored (predictions, insights,...) a single node Elasticsearch instance, used to index all logos to run ANN search for automatic logo classification 2 a Triton instance, used to serve object detection models (nutriscore, nutrition-table, universal-logo-detector) 3 . Robotoff also depends on external services in production: the MongoDB instance of Product Opener, to fetch the latest product version without querying the Product Opener API the redis instance of Product Opener, where all product updates are sent to an event queue (as a Redis Stream ) Communication between API and workers happens through Robotoff Redis DB using rq . 4 Jobs are sent through rq messaging queues. We currently have three types of queues: High-priority queues, used when a product is updated/deleted, or when a new image is uploaded. All jobs associated with a product are always sent to the same queue, based on the product barcode 5 . This way, we prevent concurrent processing for the same product (DB deadlocks or integrity errors). Low priority queue robotoff-low , which is used for all lower-priority jobs. A machine learning queue ( robotoff-ml-model ): this is used to run ML models (logo detection, ingredient extraction,...) on product images. Tasks published on this queue should not directly import insights into the database, they should only create image predictions. Insights import should be done in a separate job using high-priority queues. Currently, we have 4 workers that can process jobs on high priority queues ( robotoff-worker-{1,4} ). These workers listen to a single high priority queue each. Then can also process jobs published on the ML and low queues. By order of priority, they handle high-priority jobs first, then robotoff-ml-models jobs, then low-priority jobs. This way, we ensure low priority jobs don't use excessive system resources, by limiting the number of workers that can handle such jobs. Two workers ( robotoff-worker-ml-1 and robotoff-worker-ml-2 ) are dedicated to the ML model queue, to ensure a good throughput for ML inference. Robotoff allows to predict many information (also called insights ), mostly from the product images or OCR. Each time a contributor uploads a new image on Open Food Facts, the text on this image is extracted using Google Cloud Vision, an OCR (Optical Character Recognition) service. Robotoff receives a new event through a webhook each time this occurs, with the URLs of the image and the resulting OCR (as a JSON file). We use simple string matching algorithms to find patterns in the OCR text to generate new predictions 6 . To have a more in-depth understanding of the difference between predictions and insights, see the predictions page. We also use a ML model to extract objects from images. 7 One model tries to detect any logo 2 . Detected logos are then embedded in a vector space using the openAI pre-trained model CLIP-vit-base-patch32. In this space we use a k-nearest-neighbor approach to try to classify the logo, predicting a brand or a label. Hunger game also collects users annotations to have ground truth ( logo game ). Another model tries to detect the grade of the Nutri-Score (A to E) with a computer vision model. The above detections generate predictions which in turn generate many types of insights 8 : labels stores packager codes packaging product weight expiration date brand ... Predictions, as well as insights are stored in the PostgreSQL database. These new insights are then accessible to all annotation tools (Hunger Games, mobile apps,...), that can validate or not the insight. If the insight is validated by an authenticated user, it's applied immediately and the product is updated through Product Opener API 10 . If it's reported as invalid, no update is performed, but the insight is marked as annotated so that it is not suggested to another annotator. If the user is not authenticated, a system of votes is used (3 consistent votes trigger the insight application). Some insights with high confidence are applied automatically, 10 minutes after import. Robotoff is also notified by Product Opener every time a product is updated or deleted 9 . This is used to delete insights associated with deleted products, or to update them accordingly. See scheduler.run \u21a9 see robotoff.models.ImageAnnotation robotoff.logos \u21a9 \u21a9 see docker/ml.yml \u21a9 See robotoff.workers.queues and robotoff.workers.tasks \u21a9 See get_high_queue function in robotoff.workers.queues \u21a9 see robotoff.models.Prediction \u21a9 see robotoff.models.ImagePrediction and robotoff.workers.tasks.import_image.run_import_image_job \u21a9 see robotoff.models.ProductInsight \u21a9 see workers.tasks.product_updated and workers.tasks.delete_product_insights_job \u21a9 see robotoff.insights.annotate \u21a9","title":"Architecture"},{"location":"introduction/architecture/#architecture","text":"Robotoff is made of several services: the public API service the scheduler , responsible for launching recurrent tasks (downloading new dataset, processing insights automatically,...) 1 the workers , responsible for all long-lasting tasks a redis instance specific to Robotoff, used to handle locks and messaging queues the update listener , responsible for listening to Product Opener events and triggering actions accordingly a PostgreSQL database, where all Robotoff data are stored (predictions, insights,...) a single node Elasticsearch instance, used to index all logos to run ANN search for automatic logo classification 2 a Triton instance, used to serve object detection models (nutriscore, nutrition-table, universal-logo-detector) 3 . Robotoff also depends on external services in production: the MongoDB instance of Product Opener, to fetch the latest product version without querying the Product Opener API the redis instance of Product Opener, where all product updates are sent to an event queue (as a Redis Stream ) Communication between API and workers happens through Robotoff Redis DB using rq . 4 Jobs are sent through rq messaging queues. We currently have three types of queues: High-priority queues, used when a product is updated/deleted, or when a new image is uploaded. All jobs associated with a product are always sent to the same queue, based on the product barcode 5 . This way, we prevent concurrent processing for the same product (DB deadlocks or integrity errors). Low priority queue robotoff-low , which is used for all lower-priority jobs. A machine learning queue ( robotoff-ml-model ): this is used to run ML models (logo detection, ingredient extraction,...) on product images. Tasks published on this queue should not directly import insights into the database, they should only create image predictions. Insights import should be done in a separate job using high-priority queues. Currently, we have 4 workers that can process jobs on high priority queues ( robotoff-worker-{1,4} ). These workers listen to a single high priority queue each. Then can also process jobs published on the ML and low queues. By order of priority, they handle high-priority jobs first, then robotoff-ml-models jobs, then low-priority jobs. This way, we ensure low priority jobs don't use excessive system resources, by limiting the number of workers that can handle such jobs. Two workers ( robotoff-worker-ml-1 and robotoff-worker-ml-2 ) are dedicated to the ML model queue, to ensure a good throughput for ML inference. Robotoff allows to predict many information (also called insights ), mostly from the product images or OCR. Each time a contributor uploads a new image on Open Food Facts, the text on this image is extracted using Google Cloud Vision, an OCR (Optical Character Recognition) service. Robotoff receives a new event through a webhook each time this occurs, with the URLs of the image and the resulting OCR (as a JSON file). We use simple string matching algorithms to find patterns in the OCR text to generate new predictions 6 . To have a more in-depth understanding of the difference between predictions and insights, see the predictions page. We also use a ML model to extract objects from images. 7 One model tries to detect any logo 2 . Detected logos are then embedded in a vector space using the openAI pre-trained model CLIP-vit-base-patch32. In this space we use a k-nearest-neighbor approach to try to classify the logo, predicting a brand or a label. Hunger game also collects users annotations to have ground truth ( logo game ). Another model tries to detect the grade of the Nutri-Score (A to E) with a computer vision model. The above detections generate predictions which in turn generate many types of insights 8 : labels stores packager codes packaging product weight expiration date brand ... Predictions, as well as insights are stored in the PostgreSQL database. These new insights are then accessible to all annotation tools (Hunger Games, mobile apps,...), that can validate or not the insight. If the insight is validated by an authenticated user, it's applied immediately and the product is updated through Product Opener API 10 . If it's reported as invalid, no update is performed, but the insight is marked as annotated so that it is not suggested to another annotator. If the user is not authenticated, a system of votes is used (3 consistent votes trigger the insight application). Some insights with high confidence are applied automatically, 10 minutes after import. Robotoff is also notified by Product Opener every time a product is updated or deleted 9 . This is used to delete insights associated with deleted products, or to update them accordingly. See scheduler.run \u21a9 see robotoff.models.ImageAnnotation robotoff.logos \u21a9 \u21a9 see docker/ml.yml \u21a9 See robotoff.workers.queues and robotoff.workers.tasks \u21a9 See get_high_queue function in robotoff.workers.queues \u21a9 see robotoff.models.Prediction \u21a9 see robotoff.models.ImagePrediction and robotoff.workers.tasks.import_image.run_import_image_job \u21a9 see robotoff.models.ProductInsight \u21a9 see workers.tasks.product_updated and workers.tasks.delete_product_insights_job \u21a9 see robotoff.insights.annotate \u21a9","title":"Architecture"},{"location":"introduction/contributing/","text":"Contributing Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions Report Bugs Report bugs at https://github.com/openfoodfacts/robotoff/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs Look through the Github issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Issues tagged with \"good first issue\" are suitable for newcomers. Implement Features Look through the Github issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation Robotoff could always use more documentation, whether as part of the official Robotoff docs or in docstrings. Submit Feedback The best way to send feedback is to file an issue at https://github.com/openfoodfacts/robotoff/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome. Get Started! Ready to contribute code? Here's how to set up Robotoff for local development. Fork the robotoff repo on Github. Clone your fork locally: git clone git@github.com:your_name_here/robotoff.git Robotoff uses git lfs to store binary files necessary for it to work. To setup git lfs: Install git lfs go to robotoff directory and setup git lfs with git lfs install . This only has to be done once. Fetch LFS files with git lfs fetch && git lfs checkout choose between docker install (recommended) or local install and run it. code! When you're done making changes, check that your changes pass flake8, mypy and the tests. In addition, ensure that your code is formatted using black: If you are on Windows, make sure you have Make for Windows installed. Don't forget to add its path in your system environment variables . A sample path may look like this: C:\\Program Files (x86)\\GnuWin32\\bin It is recommended to use Window's default command prompt instead of Power shell for smooth installation. If you are using docker: make lint make checks make tests To test the APIs on your localhost run docker compose up You can make a post request through Postman or simply paste the url in a web browser to make a get request like this one http://localhost:5500/api/v1/insights/ The mapping of functions and API path is at the end of robotoff/app/api.py If you are on a local install: flake8 black --check . mypy . isort --check . uv run pytest tests Before running the test cases make sure you have a database created. Have a look at .env and robotoff/settings.py the default database name, user, and password is: postgres Configure them through environment (you may use .env if you use docker) as you like. See dev install for more information about how to restore database dumps. Commit your changes and push your branch to Github: git status git add files-you-have-modified git commit -m \"fix: your detailed description of your changes\" git push origin name-of-your-bugfix-or-feature In brief, commit messages should follow these conventions: we follow Conventional Commits specification, please prefix your commit messages with fix: , feat: ,... Always contain a subject line which briefly describes the changes made. For example \"docs: update CONTRIBUTING.rst\". Subject lines should not exceed 50 characters. The commit body should contain context about the change - how the code worked before, how it works now and why you decided to solve the issue in the way you did. More tips at https://chris.beams.io/posts/git-commit Submit a pull request through the Github website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring. The pull request should work for Python 3.11. Check https://github.com/openfoodfacts/robotoff/actions and make sure that the tests pass for all supported Python versions. This contributing page was adapted from Pyswarms documentation .","title":"Contributing"},{"location":"introduction/contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"introduction/contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"introduction/contributing/#report-bugs","text":"Report bugs at https://github.com/openfoodfacts/robotoff/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"introduction/contributing/#fix-bugs","text":"Look through the Github issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Issues tagged with \"good first issue\" are suitable for newcomers.","title":"Fix Bugs"},{"location":"introduction/contributing/#implement-features","text":"Look through the Github issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"introduction/contributing/#write-documentation","text":"Robotoff could always use more documentation, whether as part of the official Robotoff docs or in docstrings.","title":"Write Documentation"},{"location":"introduction/contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/openfoodfacts/robotoff/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome.","title":"Submit Feedback"},{"location":"introduction/contributing/#get-started","text":"Ready to contribute code? Here's how to set up Robotoff for local development. Fork the robotoff repo on Github. Clone your fork locally: git clone git@github.com:your_name_here/robotoff.git Robotoff uses git lfs to store binary files necessary for it to work. To setup git lfs: Install git lfs go to robotoff directory and setup git lfs with git lfs install . This only has to be done once. Fetch LFS files with git lfs fetch && git lfs checkout choose between docker install (recommended) or local install and run it. code! When you're done making changes, check that your changes pass flake8, mypy and the tests. In addition, ensure that your code is formatted using black: If you are on Windows, make sure you have Make for Windows installed. Don't forget to add its path in your system environment variables . A sample path may look like this: C:\\Program Files (x86)\\GnuWin32\\bin It is recommended to use Window's default command prompt instead of Power shell for smooth installation. If you are using docker: make lint make checks make tests To test the APIs on your localhost run docker compose up You can make a post request through Postman or simply paste the url in a web browser to make a get request like this one http://localhost:5500/api/v1/insights/ The mapping of functions and API path is at the end of robotoff/app/api.py If you are on a local install: flake8 black --check . mypy . isort --check . uv run pytest tests Before running the test cases make sure you have a database created. Have a look at .env and robotoff/settings.py the default database name, user, and password is: postgres Configure them through environment (you may use .env if you use docker) as you like. See dev install for more information about how to restore database dumps. Commit your changes and push your branch to Github: git status git add files-you-have-modified git commit -m \"fix: your detailed description of your changes\" git push origin name-of-your-bugfix-or-feature In brief, commit messages should follow these conventions: we follow Conventional Commits specification, please prefix your commit messages with fix: , feat: ,... Always contain a subject line which briefly describes the changes made. For example \"docs: update CONTRIBUTING.rst\". Subject lines should not exceed 50 characters. The commit body should contain context about the change - how the code worked before, how it works now and why you decided to solve the issue in the way you did. More tips at https://chris.beams.io/posts/git-commit Submit a pull request through the Github website.","title":"Get Started!"},{"location":"introduction/contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring. The pull request should work for Python 3.11. Check https://github.com/openfoodfacts/robotoff/actions and make sure that the tests pass for all supported Python versions. This contributing page was adapted from Pyswarms documentation .","title":"Pull Request Guidelines"},{"location":"references/batch-job/","text":"Google Batch Job Robotoff primarily provides models to Open Food Facts with real-time inference using Nvidia Triton Inference on GPU (gpu-01 server). However, this approach presents two major gaps: Challenges in processing large volumes of products during extended job runs Limited access to larger computing resources To fill these gaps, we integrated a batch job feature into Robotoff, leveraging the capabilities of Google Cloud Platform. Architecture The batch job pipeline is structured as follow: 1. Launch job The role of this command is to prepare and launch the job in the cloud. The launch depends on the type of job to perform, such as ingredients-spellcheck . Therefore, it takes as parameter job_type . Depending on the job type, the command will be responsible of: Generate the google credentials from the production environment variables, Extracting, preparing and storing the data to process, Query the config file relative to the job and validate it using Pydantic , Launch the google batch job. The command can be found as a Command launch_batch_job in the CLI directory 1 . 2. Config files The configuration file define the resources and setup allocated to the google batch job. Each batch job requires an unique configuration, stored as a YAML file 2 . It contains: The resources location, The type and number of resources allocated, The maximal run duration, The number of tasks in parallel, The number of retries, ... When initiating a job, the configuration is validated using the Pydantic library. This process serves two purposes: Prevents errors that could potentially cause the pipeline to fail, Safeguards against the allocation of unnecessarily expensive resources. For more information about Google Batch Job configuration, check the official documentation . 3. The container registry The container registry represents the core of the batch job. It contains the required dependencies and algorithms. Docker images are maintained independently of the Robotoff micro-service 3 . Each directory contains the files with their Dockerfile . Once built, the Docker image is pushed manually using the Make command written in the Makefile, such as deploy-spellcheck . The container needs to be accessible from the batch job once launched. Can be used as registry: Google Artifact Registry within the project Robotoff , Docker hub, Public GitHub repository, such as Robotoff . 4. Batch job Once launched, the batch job goes throught different stages: SCHEDULED, QUEUED, RUNNING, SUCCEEDED, or FAILED. Each batch job is identified as the job type name associated with the launch datetime . During the run, all logs are stored in the Batch Job logs file. The list of batch jobs are located in the Robotoff Google Batch Job . 5. Storage If the batch job requires to import or export data, we use a storage feature such as Google Storage as an interface between Robotoff and the job running in the cloud. If Google Storage is used, Google credentials are necessary on the Robotoff side. On the other side, since the Batch Job utilizes the default service account associated with the project Robotoff , no additional setup is required. 6. Import processed data Once the job is successfully finished, the Robotoff API endpoint is queried from the job run through with an HTTP request. The role of this endpoint is to load the data processed by the batch job and import the new predictions to the Robotoff database. Check this page to understand the process Robotoff used to transform raw Predictions into Insights . Since this endpoint has only the vocation of importing the batch job results, it is secured with a BATCH_JOB_KEY from external requests. This secured endpoint follows a Bearer Authentication . The key is set as an environment variable in Robotoff and is defined as batch environment variable during a job launch. Each batch job has its own method to import, or not, the results of the batch job. Roles To launch a batch job and import its results, the following roles needs to be set up: Artifact Registry Editor : to push Docker Image to the project image registry Batch Job Editor Service Account User Storage Admin For production, it is preferable to create a custom Service account with these roles. Additional notes Links Check the official Google Batch Job documentation: Batch Job , Google Batch Job Python API , Batch job with Python examples , Trials and errors notes Netherland (europe-west4) has GPUs (A100, L4) Add custom storage capacity to host the heavy docker image (~24GB) by adding BootDisk 1000 products processed: 1:30min (g2-instance-with 8) (overall batch job: 3:25min): L4: g2-instance-8 hourly cost: $0.896306 ==> ~ 0.05$ to process batch of 1000 A100: a2-highgpu-1g: $3.748064 A100/Cuda doesn't support FP8 A100 has less availability than L4: need to wait for batch job (can be long) or switch to us-east location Don't forget to enable Batch & Storage API if used without gcloud ( link ) see ./robotoff/cli/main.py \u21a9 see ./robotoff/batch/configs/job_configs \u21a9 see ./batch/ \u21a9","title":"Google Batch Job"},{"location":"references/batch-job/#google-batch-job","text":"Robotoff primarily provides models to Open Food Facts with real-time inference using Nvidia Triton Inference on GPU (gpu-01 server). However, this approach presents two major gaps: Challenges in processing large volumes of products during extended job runs Limited access to larger computing resources To fill these gaps, we integrated a batch job feature into Robotoff, leveraging the capabilities of Google Cloud Platform.","title":"Google Batch Job"},{"location":"references/batch-job/#architecture","text":"The batch job pipeline is structured as follow:","title":"Architecture"},{"location":"references/batch-job/#1-launch-job","text":"The role of this command is to prepare and launch the job in the cloud. The launch depends on the type of job to perform, such as ingredients-spellcheck . Therefore, it takes as parameter job_type . Depending on the job type, the command will be responsible of: Generate the google credentials from the production environment variables, Extracting, preparing and storing the data to process, Query the config file relative to the job and validate it using Pydantic , Launch the google batch job. The command can be found as a Command launch_batch_job in the CLI directory 1 .","title":"1. Launch job"},{"location":"references/batch-job/#2-config-files","text":"The configuration file define the resources and setup allocated to the google batch job. Each batch job requires an unique configuration, stored as a YAML file 2 . It contains: The resources location, The type and number of resources allocated, The maximal run duration, The number of tasks in parallel, The number of retries, ... When initiating a job, the configuration is validated using the Pydantic library. This process serves two purposes: Prevents errors that could potentially cause the pipeline to fail, Safeguards against the allocation of unnecessarily expensive resources. For more information about Google Batch Job configuration, check the official documentation .","title":"2. Config files"},{"location":"references/batch-job/#3-the-container-registry","text":"The container registry represents the core of the batch job. It contains the required dependencies and algorithms. Docker images are maintained independently of the Robotoff micro-service 3 . Each directory contains the files with their Dockerfile . Once built, the Docker image is pushed manually using the Make command written in the Makefile, such as deploy-spellcheck . The container needs to be accessible from the batch job once launched. Can be used as registry: Google Artifact Registry within the project Robotoff , Docker hub, Public GitHub repository, such as Robotoff .","title":"3. The container registry"},{"location":"references/batch-job/#4-batch-job","text":"Once launched, the batch job goes throught different stages: SCHEDULED, QUEUED, RUNNING, SUCCEEDED, or FAILED. Each batch job is identified as the job type name associated with the launch datetime . During the run, all logs are stored in the Batch Job logs file. The list of batch jobs are located in the Robotoff Google Batch Job .","title":"4. Batch job"},{"location":"references/batch-job/#5-storage","text":"If the batch job requires to import or export data, we use a storage feature such as Google Storage as an interface between Robotoff and the job running in the cloud. If Google Storage is used, Google credentials are necessary on the Robotoff side. On the other side, since the Batch Job utilizes the default service account associated with the project Robotoff , no additional setup is required.","title":"5. Storage"},{"location":"references/batch-job/#6-import-processed-data","text":"Once the job is successfully finished, the Robotoff API endpoint is queried from the job run through with an HTTP request. The role of this endpoint is to load the data processed by the batch job and import the new predictions to the Robotoff database. Check this page to understand the process Robotoff used to transform raw Predictions into Insights . Since this endpoint has only the vocation of importing the batch job results, it is secured with a BATCH_JOB_KEY from external requests. This secured endpoint follows a Bearer Authentication . The key is set as an environment variable in Robotoff and is defined as batch environment variable during a job launch. Each batch job has its own method to import, or not, the results of the batch job.","title":"6. Import processed data"},{"location":"references/batch-job/#roles","text":"To launch a batch job and import its results, the following roles needs to be set up: Artifact Registry Editor : to push Docker Image to the project image registry Batch Job Editor Service Account User Storage Admin For production, it is preferable to create a custom Service account with these roles.","title":"Roles"},{"location":"references/batch-job/#additional-notes","text":"","title":"Additional notes"},{"location":"references/batch-job/#links","text":"Check the official Google Batch Job documentation: Batch Job , Google Batch Job Python API , Batch job with Python examples ,","title":"Links"},{"location":"references/batch-job/#trials-and-errors-notes","text":"Netherland (europe-west4) has GPUs (A100, L4) Add custom storage capacity to host the heavy docker image (~24GB) by adding BootDisk 1000 products processed: 1:30min (g2-instance-with 8) (overall batch job: 3:25min): L4: g2-instance-8 hourly cost: $0.896306 ==> ~ 0.05$ to process batch of 1000 A100: a2-highgpu-1g: $3.748064 A100/Cuda doesn't support FP8 A100 has less availability than L4: need to wait for batch job (can be long) or switch to us-east location Don't forget to enable Batch & Storage API if used without gcloud ( link ) see ./robotoff/cli/main.py \u21a9 see ./robotoff/batch/configs/job_configs \u21a9 see ./batch/ \u21a9","title":"Trials and errors notes"},{"location":"references/ingredients-spellcheck/","text":"Ingredients Spellcheck A key element of the Open Food Facts database is the parsing of the product ingredients. These lists of ingredients either come from contributors' annotations or from OCR-extracted text from packaging pictures. However, text typos or wrong OCR-extraction lead to ingredients not recognized by the Product Opener service. Check about this process in the wiki . For this reason, the Ingredients Spellcheck was developed to be implemented to solve this issue and improve the ingredient parsing quality. TL;DR Mistral-7B-Base was fine-tuned on lists of ingredients extracted from the Open Food Facts database. This dataset was synthetically generated using closed-source LLMs (GPT-3.5-Turbo) and manually reviewed with Argilla, an open-source annotation tool. The current model (v1) shows the best performances over the closed-source LLMs on our benchmark . A custom evaluation algorithm was created to correctly estimate the Spellcheck performances. Model Correction Precision Correction Recall Correction F1 GPT-3.5-Turbo 0.557 0.727 0.631 GPT-4o 0.311 0.702 0.431 Gemini-1.5-flash 0.544 0.596 0.569 Claude3-Sonnet-3.5 0.178 0.810 0.292 Our model 0.664 0.630 0.647 The model is integrated into Robotoff in Batch Inference using Google Batch Job. Evaluation algorithm Our solution is very specific: correct errors in list of ingredients to enable the Ingredients Parser to accurately identify the composition of each product. However, since the corrections are later added to the database, we need to ensure the model doesn't correct an ingredient by mistake. In other words, we minimize the number of False Positives while maximizing the overall Recall. Traditional evaluation metrics, such as ROUGE , BLEU , or METEOR fall short in assessing the quality of the spellcheck process. They don't provide a detailed analysis about how many words were correctly rectified versus those that weren't... Therefore, we developed an algorithm that takes 3 inputs: the original, the reference, and the prediction of a list of ingredients. Example: Original: \"Th cat si on the fride,\" Reference: \"The cat is on the fridge.\" Prediction: \"Th big cat is in the fridge.\" We transform each text into a sequence of tokens and perform a sequence alignment method to align identical tokens between respectively original-reference, and prediction-reference. We assign 1 or 0 whether the tokens is modified. By comparing these 2 pairs of sequences, we calculate the number of True Positives (TP), False Positives (FP), and True Negatives (TN). Therefore, the overall Precision and Recall. Orig-Ref: 1 0 0 1 0 1 1 1 1 Orig-Pred: 0 1 0 1 1 1 1 1 1 Signification: FN FP TN TP FP TP TP TP TP Coupled with a benchmark carefully prepared using the Spellcheck Guidelines , the algorithm is capable of evaluating any solution, from Regular Expression techniques to LLMs. You'll find more details about the evaluation algorithm 1 in the project README . Guidelines The Guidelines is a set of rules defined to guide and restrict the correction made by the Spellcheck. It was also used to create the benchmark , and also to generate the training dataset using proprietary LLMs (GPT-3.5-Turbo) for the synthetic data generation. Model The model is accessible on Hugging Face , along its demo . A text instruction is provided to the model during the training and inference, which you can find in the same model repository. Training pipeline The model training consists in a succession of steps, each one requiring different resources allocations, such as cloud GPUs, data validation and logging. For this reason, we decided to orchestrate the training using Metaflow , an orchestrator designed for Data science and Machine Learning projects. The training pipeline 2 is composed as follow: Configurations and hyperparameters are imported to the pipeline from config yaml files 3 . The training job is launched in the cloud using AWS Sagemaker . The spellcheck/src/ package, containing the different modules, is imported as well as the training script 4 . Once the job done, the model artifact is stored in AWS S3 bucket (private). All training details are tracked in the Experiment Tracker Comet ML . The fine-tuned model is then evaluated on the benchmark using the custom evaluation algorithm . vLLM is used to accelerate the evaluation. Currently, this process is handled manually, but further work is needed to fully integrate it into the pipeline. The predictions against the benchmark, also stored in AWS S3, are sent to Argilla for human-evaluation 5 under an unique ID: the experiment key . Human-evaluation with Argilla The model and dataset versions are handled by Hugging Face repository as branch (v1, v2) and commits (v1.1, v1.2). You can easily access any version using the Dataset library from Hugging Face. from datasets import load_dataset dataset = load_dataset ( path = \"openfoodfacts/spellcheck-dataset\" , revision = \"v8\" , split = \"train+test\" ) Integration with Batch Job Once the model is selected, the inference script with its dependencies are containerized in a Docker Image 6 before being pushed to the Image Registry 7 (currently Google Artifact Registry). The image is then used within the batch job pipeline , defined by the batch job type ingredients-spellcheck . see spellcheck/src/spellcheck/evaluation/evaluator \u21a9 see scripts/dags \u21a9 see spellcheck/config/training \u21a9 see spellcheck/scripts/training \u21a9 see spellcheck/src/spellcheck/argilla \u21a9 see robotoff/batch/spellcheck \u21a9 see robotoff/makefile \u21a9","title":"Ingredients Spellcheck"},{"location":"references/ingredients-spellcheck/#ingredients-spellcheck","text":"A key element of the Open Food Facts database is the parsing of the product ingredients. These lists of ingredients either come from contributors' annotations or from OCR-extracted text from packaging pictures. However, text typos or wrong OCR-extraction lead to ingredients not recognized by the Product Opener service. Check about this process in the wiki . For this reason, the Ingredients Spellcheck was developed to be implemented to solve this issue and improve the ingredient parsing quality.","title":"Ingredients Spellcheck"},{"location":"references/ingredients-spellcheck/#tldr","text":"Mistral-7B-Base was fine-tuned on lists of ingredients extracted from the Open Food Facts database. This dataset was synthetically generated using closed-source LLMs (GPT-3.5-Turbo) and manually reviewed with Argilla, an open-source annotation tool. The current model (v1) shows the best performances over the closed-source LLMs on our benchmark . A custom evaluation algorithm was created to correctly estimate the Spellcheck performances. Model Correction Precision Correction Recall Correction F1 GPT-3.5-Turbo 0.557 0.727 0.631 GPT-4o 0.311 0.702 0.431 Gemini-1.5-flash 0.544 0.596 0.569 Claude3-Sonnet-3.5 0.178 0.810 0.292 Our model 0.664 0.630 0.647 The model is integrated into Robotoff in Batch Inference using Google Batch Job.","title":"TL;DR"},{"location":"references/ingredients-spellcheck/#evaluation-algorithm","text":"Our solution is very specific: correct errors in list of ingredients to enable the Ingredients Parser to accurately identify the composition of each product. However, since the corrections are later added to the database, we need to ensure the model doesn't correct an ingredient by mistake. In other words, we minimize the number of False Positives while maximizing the overall Recall. Traditional evaluation metrics, such as ROUGE , BLEU , or METEOR fall short in assessing the quality of the spellcheck process. They don't provide a detailed analysis about how many words were correctly rectified versus those that weren't... Therefore, we developed an algorithm that takes 3 inputs: the original, the reference, and the prediction of a list of ingredients. Example: Original: \"Th cat si on the fride,\" Reference: \"The cat is on the fridge.\" Prediction: \"Th big cat is in the fridge.\" We transform each text into a sequence of tokens and perform a sequence alignment method to align identical tokens between respectively original-reference, and prediction-reference. We assign 1 or 0 whether the tokens is modified. By comparing these 2 pairs of sequences, we calculate the number of True Positives (TP), False Positives (FP), and True Negatives (TN). Therefore, the overall Precision and Recall. Orig-Ref: 1 0 0 1 0 1 1 1 1 Orig-Pred: 0 1 0 1 1 1 1 1 1 Signification: FN FP TN TP FP TP TP TP TP Coupled with a benchmark carefully prepared using the Spellcheck Guidelines , the algorithm is capable of evaluating any solution, from Regular Expression techniques to LLMs. You'll find more details about the evaluation algorithm 1 in the project README .","title":"Evaluation algorithm"},{"location":"references/ingredients-spellcheck/#guidelines","text":"The Guidelines is a set of rules defined to guide and restrict the correction made by the Spellcheck. It was also used to create the benchmark , and also to generate the training dataset using proprietary LLMs (GPT-3.5-Turbo) for the synthetic data generation.","title":"Guidelines"},{"location":"references/ingredients-spellcheck/#model","text":"The model is accessible on Hugging Face , along its demo . A text instruction is provided to the model during the training and inference, which you can find in the same model repository.","title":"Model"},{"location":"references/ingredients-spellcheck/#training-pipeline","text":"The model training consists in a succession of steps, each one requiring different resources allocations, such as cloud GPUs, data validation and logging. For this reason, we decided to orchestrate the training using Metaflow , an orchestrator designed for Data science and Machine Learning projects. The training pipeline 2 is composed as follow: Configurations and hyperparameters are imported to the pipeline from config yaml files 3 . The training job is launched in the cloud using AWS Sagemaker . The spellcheck/src/ package, containing the different modules, is imported as well as the training script 4 . Once the job done, the model artifact is stored in AWS S3 bucket (private). All training details are tracked in the Experiment Tracker Comet ML . The fine-tuned model is then evaluated on the benchmark using the custom evaluation algorithm . vLLM is used to accelerate the evaluation. Currently, this process is handled manually, but further work is needed to fully integrate it into the pipeline. The predictions against the benchmark, also stored in AWS S3, are sent to Argilla for human-evaluation 5 under an unique ID: the experiment key . Human-evaluation with Argilla The model and dataset versions are handled by Hugging Face repository as branch (v1, v2) and commits (v1.1, v1.2). You can easily access any version using the Dataset library from Hugging Face. from datasets import load_dataset dataset = load_dataset ( path = \"openfoodfacts/spellcheck-dataset\" , revision = \"v8\" , split = \"train+test\" )","title":"Training pipeline"},{"location":"references/ingredients-spellcheck/#integration-with-batch-job","text":"Once the model is selected, the inference script with its dependencies are containerized in a Docker Image 6 before being pushed to the Image Registry 7 (currently Google Artifact Registry). The image is then used within the batch job pipeline , defined by the batch job type ingredients-spellcheck . see spellcheck/src/spellcheck/evaluation/evaluator \u21a9 see scripts/dags \u21a9 see spellcheck/config/training \u21a9 see spellcheck/scripts/training \u21a9 see spellcheck/src/spellcheck/argilla \u21a9 see robotoff/batch/spellcheck \u21a9 see robotoff/makefile \u21a9","title":"Integration with Batch Job"},{"location":"references/logos-ANN/","text":"Logo-ANN About 1600 products are added every day to the database. Each product having multiple logos on its packaging, thousands of new logos are added every day to the valuable sources of information. These logos are often useful to get important data on products (origin, brand, quality, label, etc...). A logo automatic detection from images and a logo manual classification features are implemented to Robotoff. The first step is to extract logos from products images of the database. The second one is to vectorize each logo thanks to a computer vision model. The third and last one is to search for each logo its nearest neighbors in an index containing all the embedded logos. Logos extraction When a new image is added to the database, Robotoff applies an object detection model to extract logos from it. 1 This model, named \"universal-logo-detector\" 2 , is an ONNX model trained by Open Food Facts on numerous data from the database. For each input image, it returns bounding boxes that represent the detection zone of each logo of the image and the category of the logo, namely \"brand\" or \"label\". To know more about this model, see the model card on HuggingFace . Logos embedding After the detection of a logo, in the same function 1 , Robotoff uses a computer vision model to vectorize it. The model we use is CLIP-vit-base-patch32 , a model developed and trained by OpenAI. Only the vision part of the model is used here, as the objective is only to vectorize the logos. The choice of CLIP-vit-base-patch32 was made after this benchmark . The model is loaded with Triton and is used only for inference. With the logo crop of the initial image as input, CLIP returns an embedding and Robotoff stores it in its postgresql database. 3 Approximate Nearest Neighbors Logos Search Each generated embedding is stored in an ElasticSearch index for nearest neighbor search. ElasticSearch allows for approximate nearest neighbor (ANN) search with an HNSW (Hierarchical Navigable Small World) index, which leads to fast and accurate search (see ANN benchmark ). After storing the embedding in the index, a search for its nearest neighbors is performed and the IDs of these neighbors are stored in the Robotoff PostgreSQL database. The nearest neighbor search is available via an API 4 available (here)[https://robotoff.openfoodfacts.org/api/v1/ann/search/185171?count=50] and used by (Hunger Games)[https://hunger.openfoodfacts.org/], the annotation game connected to Robotoff. see robotoff.workers.tasks.import_image.run_logo_object_detection \u21a9 \u21a9 see models.universal-logo-detector \u21a9 see robotoff.workers.tasks.import_image.save_logo_embeddings \u21a9 see robotoff.app.api.ANNResource \u21a9","title":"Logo-ANN"},{"location":"references/logos-ANN/#logo-ann","text":"About 1600 products are added every day to the database. Each product having multiple logos on its packaging, thousands of new logos are added every day to the valuable sources of information. These logos are often useful to get important data on products (origin, brand, quality, label, etc...). A logo automatic detection from images and a logo manual classification features are implemented to Robotoff. The first step is to extract logos from products images of the database. The second one is to vectorize each logo thanks to a computer vision model. The third and last one is to search for each logo its nearest neighbors in an index containing all the embedded logos.","title":"Logo-ANN"},{"location":"references/logos-ANN/#logos-extraction","text":"When a new image is added to the database, Robotoff applies an object detection model to extract logos from it. 1 This model, named \"universal-logo-detector\" 2 , is an ONNX model trained by Open Food Facts on numerous data from the database. For each input image, it returns bounding boxes that represent the detection zone of each logo of the image and the category of the logo, namely \"brand\" or \"label\". To know more about this model, see the model card on HuggingFace .","title":"Logos extraction"},{"location":"references/logos-ANN/#logos-embedding","text":"After the detection of a logo, in the same function 1 , Robotoff uses a computer vision model to vectorize it. The model we use is CLIP-vit-base-patch32 , a model developed and trained by OpenAI. Only the vision part of the model is used here, as the objective is only to vectorize the logos. The choice of CLIP-vit-base-patch32 was made after this benchmark . The model is loaded with Triton and is used only for inference. With the logo crop of the initial image as input, CLIP returns an embedding and Robotoff stores it in its postgresql database. 3","title":"Logos embedding"},{"location":"references/logos-ANN/#approximate-nearest-neighbors-logos-search","text":"Each generated embedding is stored in an ElasticSearch index for nearest neighbor search. ElasticSearch allows for approximate nearest neighbor (ANN) search with an HNSW (Hierarchical Navigable Small World) index, which leads to fast and accurate search (see ANN benchmark ). After storing the embedding in the index, a search for its nearest neighbors is performed and the IDs of these neighbors are stored in the Robotoff PostgreSQL database. The nearest neighbor search is available via an API 4 available (here)[https://robotoff.openfoodfacts.org/api/v1/ann/search/185171?count=50] and used by (Hunger Games)[https://hunger.openfoodfacts.org/], the annotation game connected to Robotoff. see robotoff.workers.tasks.import_image.run_logo_object_detection \u21a9 \u21a9 see models.universal-logo-detector \u21a9 see robotoff.workers.tasks.import_image.save_logo_embeddings \u21a9 see robotoff.app.api.ANNResource \u21a9","title":"Approximate Nearest Neighbors Logos Search"},{"location":"references/models/","text":"Models An overview of all trained models, in production or not, is available in this spreadsheet .","title":"Models"},{"location":"references/models/#models","text":"An overview of all trained models, in production or not, is available in this spreadsheet .","title":"Models"},{"location":"references/package/","text":"Package reference Robotoff is deployed on a web server, but is also distributed as a library. This document presents a brief, high-level overview of Robotoff\u2019s library primary features. This guide will cover: Using OFF dataset Using the taxonomies (ingredient, label, category) Install Robotoff is currently compatible with Python 3.11. Robotoff can be installed following dev install docs Play with the Open Food Facts dataset First, download the dataset: python -m robotoff download-dataset Robotoff includes a set of tools to easily handle the OFF dataset. As an example, we can print the product name of all complete products from France that have ingredients in French with: from robotoff.products import ProductDataset ds = ProductDataset . load () product_iter = ( ds . stream () . filter_by_country_tag ( 'en:france' ) . filter_nonempty_text_field ( 'ingredients_text_fr' ) . filter_by_state_tag ( 'en:complete' ) . iter ()) for product in product_iter : print ( product [ 'product_name' ]) We first lazily load the dataset using ProductDataset.load() . Then, we create a ProductStream using the ProductDataset.stream() method, and apply filters on the stream of products. The following filters are currently available: filter_by_country_tag filter_by_state_tag filter_nonempty_text_field filter_empty_text_field filter_nonempty_tag_field filter_empty_tag_field filter_by_modified_datetime Play with the taxonomies Taxonomies contains items (such as ingredients, labels or categories) organized in a hierarchical way. Some items are children of other items. For instance, en:brown-rice is a child of en:rice . from robotoff.taxonomy import get_taxonomy # supported taxonomies: ingredient, category, label taxonomy = get_taxonomy ( 'category' ) brown_rice = taxonomy [ 'en:brown-rices' ] rice = taxonomy [ 'en:rices' ] print ( brown_rice ) # Output: <TaxonomyNode en:brown-rices> print ( brown_rice . children ) # Output: [<TaxonomyNode en:brown-jasmine-rices>, <TaxonomyNode en:brown-basmati-rices>] assert brown_rice . is_child_of ( rice ) assert rice . is_parent_of ( brown_rice ) assert brown_rice . get_localized_name ( 'fr' ) == 'Riz complet' # find_deepest_item takes a list of string as input and outputs a string deepest_node = taxonomy . find_deepest_nodes ([ rice , brown_rice ]) assert deepest_node == [ brown_rice ] print ( brown_rice . get_synonyms ( 'fr' )) # Output: ['Riz complet', 'riz cargo', 'riz brun', 'riz semi-complet'] print ( brown_rice . get_parents_hierarchy ()) # Output: [<TaxonomyNode en:rices>, <TaxonomyNode en:cereal-grains>, <TaxonomyNode en:cereals-and-their-products>, <TaxonomyNode en:cereals-and-potatoes>, <TaxonomyNode en:plant-based-foods>, <TaxonomyNode en:plant-based-foods-and-beverages>, <TaxonomyNode en:seeds>]","title":"Package reference"},{"location":"references/package/#package-reference","text":"Robotoff is deployed on a web server, but is also distributed as a library. This document presents a brief, high-level overview of Robotoff\u2019s library primary features. This guide will cover: Using OFF dataset Using the taxonomies (ingredient, label, category)","title":"Package reference"},{"location":"references/package/#install","text":"Robotoff is currently compatible with Python 3.11. Robotoff can be installed following dev install docs","title":"Install"},{"location":"references/package/#play-with-the-open-food-facts-dataset","text":"First, download the dataset: python -m robotoff download-dataset Robotoff includes a set of tools to easily handle the OFF dataset. As an example, we can print the product name of all complete products from France that have ingredients in French with: from robotoff.products import ProductDataset ds = ProductDataset . load () product_iter = ( ds . stream () . filter_by_country_tag ( 'en:france' ) . filter_nonempty_text_field ( 'ingredients_text_fr' ) . filter_by_state_tag ( 'en:complete' ) . iter ()) for product in product_iter : print ( product [ 'product_name' ]) We first lazily load the dataset using ProductDataset.load() . Then, we create a ProductStream using the ProductDataset.stream() method, and apply filters on the stream of products. The following filters are currently available: filter_by_country_tag filter_by_state_tag filter_nonempty_text_field filter_empty_text_field filter_nonempty_tag_field filter_empty_tag_field filter_by_modified_datetime","title":"Play with the Open Food Facts dataset"},{"location":"references/package/#play-with-the-taxonomies","text":"Taxonomies contains items (such as ingredients, labels or categories) organized in a hierarchical way. Some items are children of other items. For instance, en:brown-rice is a child of en:rice . from robotoff.taxonomy import get_taxonomy # supported taxonomies: ingredient, category, label taxonomy = get_taxonomy ( 'category' ) brown_rice = taxonomy [ 'en:brown-rices' ] rice = taxonomy [ 'en:rices' ] print ( brown_rice ) # Output: <TaxonomyNode en:brown-rices> print ( brown_rice . children ) # Output: [<TaxonomyNode en:brown-jasmine-rices>, <TaxonomyNode en:brown-basmati-rices>] assert brown_rice . is_child_of ( rice ) assert rice . is_parent_of ( brown_rice ) assert brown_rice . get_localized_name ( 'fr' ) == 'Riz complet' # find_deepest_item takes a list of string as input and outputs a string deepest_node = taxonomy . find_deepest_nodes ([ rice , brown_rice ]) assert deepest_node == [ brown_rice ] print ( brown_rice . get_synonyms ( 'fr' )) # Output: ['Riz complet', 'riz cargo', 'riz brun', 'riz semi-complet'] print ( brown_rice . get_parents_hierarchy ()) # Output: [<TaxonomyNode en:rices>, <TaxonomyNode en:cereal-grains>, <TaxonomyNode en:cereals-and-their-products>, <TaxonomyNode en:cereals-and-potatoes>, <TaxonomyNode en:plant-based-foods>, <TaxonomyNode en:plant-based-foods-and-beverages>, <TaxonomyNode en:seeds>]","title":"Play with the taxonomies"},{"location":"references/datasets/llm-image-extraction/","text":"LLM image extraction dataset Large visual language models (LVLMs) can be used to extract information from images. In this document, we describe the schema used for LLM image extraction datasets uploaded on Hugging Face as Parquet files. We can automatically create datasets on Hugging Face using the labelr CLI 1 . The price-tag-extraction dataset is an example of such dataset. Schema image_id : string, unique identifier of the image. For Open Food Facts images, it's usually in the format {barcode}_{imgid} (example: for https://images.openfoodfacts.org/images/products/703/801/000/5459/5.jpg , it would be 7038010005459_5 ) image : the image data. Note that we apply rotation based on EXIF metadata before saving the image. Images are also often resized to reduce dataset size and speed up training. output : the expected output text extracted from the image. It's expected to be a valid JSON string conforming to the schema (see Configuration file section below for more information). meta : metadata about the image. It's project dependent, but most frequent fields are: barcode (Open Food Facts projects only): the barcode of the product. off_image_id (Open Food Facts projects only): the imgid of the original image. image_url : the URL of the image. Configuration file To fine-tune a LVLM using the dataset, we also need: textual instructions for the vLLM to describe the task to perform on the image. a JSON schema describing the expected output format. This schema allows to specify the expected fields, their types, constraints and to provide a detailed description of each field. Both are stored in a configuration JSON file stored at the root of the dataset repository on Hugging Face, named config.json . Two keys are expected in this file: instructions and json_schema . An example of such file can be found here . Training a vLLM To train a vLLM with this type of dataset, you can use the train-unsloth package from the labelr repository. the parquet schema used can be found here . \u21a9","title":"LLM image extraction dataset"},{"location":"references/datasets/llm-image-extraction/#llm-image-extraction-dataset","text":"Large visual language models (LVLMs) can be used to extract information from images. In this document, we describe the schema used for LLM image extraction datasets uploaded on Hugging Face as Parquet files. We can automatically create datasets on Hugging Face using the labelr CLI 1 . The price-tag-extraction dataset is an example of such dataset.","title":"LLM image extraction dataset"},{"location":"references/datasets/llm-image-extraction/#schema","text":"image_id : string, unique identifier of the image. For Open Food Facts images, it's usually in the format {barcode}_{imgid} (example: for https://images.openfoodfacts.org/images/products/703/801/000/5459/5.jpg , it would be 7038010005459_5 ) image : the image data. Note that we apply rotation based on EXIF metadata before saving the image. Images are also often resized to reduce dataset size and speed up training. output : the expected output text extracted from the image. It's expected to be a valid JSON string conforming to the schema (see Configuration file section below for more information). meta : metadata about the image. It's project dependent, but most frequent fields are: barcode (Open Food Facts projects only): the barcode of the product. off_image_id (Open Food Facts projects only): the imgid of the original image. image_url : the URL of the image.","title":"Schema"},{"location":"references/datasets/llm-image-extraction/#configuration-file","text":"To fine-tune a LVLM using the dataset, we also need: textual instructions for the vLLM to describe the task to perform on the image. a JSON schema describing the expected output format. This schema allows to specify the expected fields, their types, constraints and to provide a detailed description of each field. Both are stored in a configuration JSON file stored at the root of the dataset repository on Hugging Face, named config.json . Two keys are expected in this file: instructions and json_schema . An example of such file can be found here .","title":"Configuration file"},{"location":"references/datasets/llm-image-extraction/#training-a-vllm","text":"To train a vLLM with this type of dataset, you can use the train-unsloth package from the labelr repository. the parquet schema used can be found here . \u21a9","title":"Training a vLLM"},{"location":"references/datasets/object-detection/","text":"Object detection datasets Object detection models are used in Robotoff for many tasks, including the detection of logos and nutrition tables. In this document, we describe the schema used for object detection datasets uploaded on Hugging Face as Parquet files. We now automatically create datasets on Hugging Face using the labelr CLI 1 . The universal-logo-detector dataset is an example of such dataset. Schema image_id : string, unique identifier of the image. For Open Food Facts images, it's usually in the format {barcode}_{imgid} (example: for https://images.openfoodfacts.org/images/products/703/801/000/5459/5.jpg, it would be 7038010005459_5 ) image : the image data. Note that we apply rotation based on EXIF metadata before saving the image (see Exif rotation section below for more information). \u1e81idth : the image width. height : the image height. meta : metadata about the image. It's project dependent, but most frequent fields are: - barcode (Open Food Facts projects only): the barcode of the product. - off_image_id (Open Food Facts projects only): the imgid of the original image. - image_url : the URL of the image. objects : a field containing ground-truth annotations, with the following subfields: - bbox : a list of bounding box. Each bounding box is a list in the format [y_min, x_min, y_max, x_max] in relative coordinates (value between 0 and 1), with the top-left corner as origin. Example: [[0.838, 0.689, 0.935, 0.891]] . - category_id : an list of category IDs (integers starting from 0), one for each bounding box. This field has the same length as bbox . - category_name : a list of category names, one for each bounding box. This field has the same length as bbox . Exif rotation Hugging Face stores images without applying EXIF rotation, and EXIF data is not preserved in the dataset. Label Studio provides bounding boxes based on the displayed image (after eventual EXIF rotation), so we need to apply the same transformation to the image. Note that images were NOT EXIF-rotated in object detection datasets uploaded before the universal-logo-detector dataset, so we need to reupload the dataset with correct orientation before training a new model. This was not an issue prior to universal-logo-detector model, as we used to create the ultralytics dataset using image_url field directly, and ultralytics use EXIF orientation metadata to rotate the image on the fly. Now, using the train-yolo package, we use the dataset stored on Hugging Face (including the image data) as the unique source of truth. It allows to \"freeze\" the dataset, preventing alteration when an image is deleted on Open Food Facts image server for example, and makes the dataset preprocessing before training much faster. the parquet schema used can be found here . \u21a9","title":"Object detection datasets"},{"location":"references/datasets/object-detection/#object-detection-datasets","text":"Object detection models are used in Robotoff for many tasks, including the detection of logos and nutrition tables. In this document, we describe the schema used for object detection datasets uploaded on Hugging Face as Parquet files. We now automatically create datasets on Hugging Face using the labelr CLI 1 . The universal-logo-detector dataset is an example of such dataset.","title":"Object detection datasets"},{"location":"references/datasets/object-detection/#schema","text":"image_id : string, unique identifier of the image. For Open Food Facts images, it's usually in the format {barcode}_{imgid} (example: for https://images.openfoodfacts.org/images/products/703/801/000/5459/5.jpg, it would be 7038010005459_5 ) image : the image data. Note that we apply rotation based on EXIF metadata before saving the image (see Exif rotation section below for more information). \u1e81idth : the image width. height : the image height. meta : metadata about the image. It's project dependent, but most frequent fields are: - barcode (Open Food Facts projects only): the barcode of the product. - off_image_id (Open Food Facts projects only): the imgid of the original image. - image_url : the URL of the image. objects : a field containing ground-truth annotations, with the following subfields: - bbox : a list of bounding box. Each bounding box is a list in the format [y_min, x_min, y_max, x_max] in relative coordinates (value between 0 and 1), with the top-left corner as origin. Example: [[0.838, 0.689, 0.935, 0.891]] . - category_id : an list of category IDs (integers starting from 0), one for each bounding box. This field has the same length as bbox . - category_name : a list of category names, one for each bounding box. This field has the same length as bbox .","title":"Schema"},{"location":"references/datasets/object-detection/#exif-rotation","text":"Hugging Face stores images without applying EXIF rotation, and EXIF data is not preserved in the dataset. Label Studio provides bounding boxes based on the displayed image (after eventual EXIF rotation), so we need to apply the same transformation to the image. Note that images were NOT EXIF-rotated in object detection datasets uploaded before the universal-logo-detector dataset, so we need to reupload the dataset with correct orientation before training a new model. This was not an issue prior to universal-logo-detector model, as we used to create the ultralytics dataset using image_url field directly, and ultralytics use EXIF orientation metadata to rotate the image on the fly. Now, using the train-yolo package, we use the dataset stored on Hugging Face (including the image data) as the unique source of truth. It allows to \"freeze\" the dataset, preventing alteration when an image is deleted on Open Food Facts image server for example, and makes the dataset preprocessing before training much faster. the parquet schema used can be found here . \u21a9","title":"Exif rotation"},{"location":"references/predictions/category-prediction/","text":"Category Prediction Knowing the category of each product is critically important at Open Food Facts, as category is used to compute Nutriscore, to assess the environmental impact of the product (thanks to Agribalyse database), to compare the product to similar ones,... In Open Food Facts, more 12,500 categories exist in the category taxonomy (as of March 2023). Category prediction using product meta-data was one the first project developed as part of Robotoff in 2018. A neural network model is used to predict categories 1 . Details about the model training, results and model assets are available on the model page on HuggingFace . This model takes as inputs (all inputs are optional): the product name ( product_name field) the ingredient list ( ingredients field): only the ingredients of depth one are considered (sub-ingredients are ignored) the ingredients extracted from OCR texts: all OCR results are fetched and ingredients present in the taxonomy are detected using flashtext library . the most common nutriments: salt, proteins, fats, saturated fats, carbohydrates, energy,... up to the most 10 recent images: we generate an embedding for each image using clip-vit-base-patch32 model, and generate a single vector using a multi-head attention layer + GlobalAveragePooling1d. The model was trained to predict a subset of all categories: broad categories (such as plant based foods and beverages) were discarded to keep only the most informative categories, and categories with less than 10 products were ignored. The model can predict categories for about 3,500 categories of the taxonomy. For each predicted category, we also fetch the prediction score of parents, children and siblings (node with the same parents) categories in the taxonomy. This will be used soon to display predictions about neighbor categories and select a more (or less) specific category on Hunger Games if needed. We also computed for each category the detection threshold above which we have a precision >= 0.99 on the split obtained from merging validation and test sets. For these predictions, we have a very high confidence that the predicted category is correct. We always generate insights from these above-threshold predictions (except if the category is already filled in for the product), and the v3_categorizer_automatic_processing campaign is added to the insight. The Hunger Game annotation campaign can be accessed here . If the experiment is successful ( precision >= 0.99 on Hunger Games questions), we will turn on automatic categorization on above-threshold predictions . Changelog Here is a summary on the milestones in category detection: 2018-12 | Deployment of the first \"matching\" categorizer based on Elasticsearch 2019-02 | Deployment of the first hierarchical category classifier using scikit-learn 2019-11 | Deployment of the first neural model (v1) for product categorization, hierarchical classification was disabled. 2021-12 | Deployment of the v2 model 2022-01 | Automatic processing of all category predictions with score >= 0.9 2022-03 | Disable automatic processing of categories 2022-10 | Remove Elasticsearch-based category predictor, switch to custom model in Robotoff codebase 2023-03 | Deployment of the v3 model 2023-08 | Disabling of the matcher predictor: after an analysis through Hunger Games, most errors were due to the matcher predictor, and the neural predictor gave most of the time accurate predictions for products for which the matcher predictor failed. see robotoff.prediction.category.neural \u21a9","title":"Category Prediction"},{"location":"references/predictions/category-prediction/#category-prediction","text":"Knowing the category of each product is critically important at Open Food Facts, as category is used to compute Nutriscore, to assess the environmental impact of the product (thanks to Agribalyse database), to compare the product to similar ones,... In Open Food Facts, more 12,500 categories exist in the category taxonomy (as of March 2023). Category prediction using product meta-data was one the first project developed as part of Robotoff in 2018. A neural network model is used to predict categories 1 . Details about the model training, results and model assets are available on the model page on HuggingFace . This model takes as inputs (all inputs are optional): the product name ( product_name field) the ingredient list ( ingredients field): only the ingredients of depth one are considered (sub-ingredients are ignored) the ingredients extracted from OCR texts: all OCR results are fetched and ingredients present in the taxonomy are detected using flashtext library . the most common nutriments: salt, proteins, fats, saturated fats, carbohydrates, energy,... up to the most 10 recent images: we generate an embedding for each image using clip-vit-base-patch32 model, and generate a single vector using a multi-head attention layer + GlobalAveragePooling1d. The model was trained to predict a subset of all categories: broad categories (such as plant based foods and beverages) were discarded to keep only the most informative categories, and categories with less than 10 products were ignored. The model can predict categories for about 3,500 categories of the taxonomy. For each predicted category, we also fetch the prediction score of parents, children and siblings (node with the same parents) categories in the taxonomy. This will be used soon to display predictions about neighbor categories and select a more (or less) specific category on Hunger Games if needed. We also computed for each category the detection threshold above which we have a precision >= 0.99 on the split obtained from merging validation and test sets. For these predictions, we have a very high confidence that the predicted category is correct. We always generate insights from these above-threshold predictions (except if the category is already filled in for the product), and the v3_categorizer_automatic_processing campaign is added to the insight. The Hunger Game annotation campaign can be accessed here . If the experiment is successful ( precision >= 0.99 on Hunger Games questions), we will turn on automatic categorization on above-threshold predictions .","title":"Category Prediction"},{"location":"references/predictions/category-prediction/#changelog","text":"Here is a summary on the milestones in category detection: 2018-12 | Deployment of the first \"matching\" categorizer based on Elasticsearch 2019-02 | Deployment of the first hierarchical category classifier using scikit-learn 2019-11 | Deployment of the first neural model (v1) for product categorization, hierarchical classification was disabled. 2021-12 | Deployment of the v2 model 2022-01 | Automatic processing of all category predictions with score >= 0.9 2022-03 | Disable automatic processing of categories 2022-10 | Remove Elasticsearch-based category predictor, switch to custom model in Robotoff codebase 2023-03 | Deployment of the v3 model 2023-08 | Disabling of the matcher predictor: after an analysis through Hunger Games, most errors were due to the matcher predictor, and the neural predictor gave most of the time accurate predictions for products for which the matcher predictor failed. see robotoff.prediction.category.neural \u21a9","title":"Changelog"},{"location":"references/predictions/image-flagging/","text":"Image flagging The image flagging system automatically identifies inappropriate or problematic content in product images to help maintain Open Food Facts' image quality standards. How it works Image flagging uses multiple detection methods to identify content that may not be appropriate for a food database: Face Detection \u2013 Uses Google Cloud Vision's Face Detection API to identify images containing human faces. Label Annotation \u2013 Scans for labels indicating the presence of humans, pets, electronics, or other non-food items. Safe Search \u2013 Uses Google Cloud Vision's Safe Search API to detect adult content or violence. Text Detection \u2013 Analyzes OCR text for keywords related to beauty products or other inappropriate content. When flagged content is detected, an image_flag prediction is generated with details about the issue and the associated confidence level. These predictions trigger notifications to moderation services where humans can review potentially problematic images. Detection Methods Face Detection The system processes faceAnnotations from Google Cloud Vision to detect human faces. If multiple faces are detected, the one with the highest confidence score is used. Only faces with a detection confidence \u2265 0.6 are flagged to minimize false positives. Prediction data includes: type : \"face_annotation\" label : \"face\" likelihood : Detection confidence score Label Annotation Detection The system flags images containing specific labels from Google Cloud Vision with confidence scores \u2265 0.6. Only the first matching label is flagged per image. Human-related labels : Face, Head, Selfie, Hair, Forehead, Chin, Cheek Arm, Tooth, Human Leg, Ankle, Eyebrow, Ear, Neck, Jaw, Nose Facial Expression, Glasses, Eyewear Child, Baby, Human Other flagged labels : Pets : Dog, Cat Technology : Computer, Laptop, Refrigerator Clothing : Jeans, Shoe The prediction data includes: type : \"label_annotation\" label : The detected label (lowercase) likelihood : Label confidence score Safe Search Detection The Safe Search API flags the following categories only if marked as \"VERY_LIKELY\": Adult content \u2013 Sexually explicit material Violence \u2013 Graphic or violent imagery The prediction data includes: type : \"safe_search_annotation\" label : \"adult\" or \"violence\" likelihood : Likelihood level name Text-based Detection The system scans OCR-extracted text for keywords from predefined keyword files. Only the first matching keyword is flagged per image. Beauty products \u2013 Cosmetic-related terms from beauty keyword file Miscellaneous \u2013 Other inappropriate content keywords from miscellaneous keyword file The prediction data includes: type : \"text\" label : \"beauty\" or \"miscellaneous\" text : The matched text phrase","title":"Image flagging"},{"location":"references/predictions/image-flagging/#image-flagging","text":"The image flagging system automatically identifies inappropriate or problematic content in product images to help maintain Open Food Facts' image quality standards.","title":"Image flagging"},{"location":"references/predictions/image-flagging/#how-it-works","text":"Image flagging uses multiple detection methods to identify content that may not be appropriate for a food database: Face Detection \u2013 Uses Google Cloud Vision's Face Detection API to identify images containing human faces. Label Annotation \u2013 Scans for labels indicating the presence of humans, pets, electronics, or other non-food items. Safe Search \u2013 Uses Google Cloud Vision's Safe Search API to detect adult content or violence. Text Detection \u2013 Analyzes OCR text for keywords related to beauty products or other inappropriate content. When flagged content is detected, an image_flag prediction is generated with details about the issue and the associated confidence level. These predictions trigger notifications to moderation services where humans can review potentially problematic images.","title":"How it works"},{"location":"references/predictions/image-flagging/#detection-methods","text":"","title":"Detection Methods"},{"location":"references/predictions/image-flagging/#face-detection","text":"The system processes faceAnnotations from Google Cloud Vision to detect human faces. If multiple faces are detected, the one with the highest confidence score is used. Only faces with a detection confidence \u2265 0.6 are flagged to minimize false positives. Prediction data includes: type : \"face_annotation\" label : \"face\" likelihood : Detection confidence score","title":"Face Detection"},{"location":"references/predictions/image-flagging/#label-annotation-detection","text":"The system flags images containing specific labels from Google Cloud Vision with confidence scores \u2265 0.6. Only the first matching label is flagged per image. Human-related labels : Face, Head, Selfie, Hair, Forehead, Chin, Cheek Arm, Tooth, Human Leg, Ankle, Eyebrow, Ear, Neck, Jaw, Nose Facial Expression, Glasses, Eyewear Child, Baby, Human Other flagged labels : Pets : Dog, Cat Technology : Computer, Laptop, Refrigerator Clothing : Jeans, Shoe The prediction data includes: type : \"label_annotation\" label : The detected label (lowercase) likelihood : Label confidence score","title":"Label Annotation Detection"},{"location":"references/predictions/image-flagging/#safe-search-detection","text":"The Safe Search API flags the following categories only if marked as \"VERY_LIKELY\": Adult content \u2013 Sexually explicit material Violence \u2013 Graphic or violent imagery The prediction data includes: type : \"safe_search_annotation\" label : \"adult\" or \"violence\" likelihood : Likelihood level name","title":"Safe Search Detection"},{"location":"references/predictions/image-flagging/#text-based-detection","text":"The system scans OCR-extracted text for keywords from predefined keyword files. Only the first matching keyword is flagged per image. Beauty products \u2013 Cosmetic-related terms from beauty keyword file Miscellaneous \u2013 Other inappropriate content keywords from miscellaneous keyword file The prediction data includes: type : \"text\" label : \"beauty\" or \"miscellaneous\" text : The matched text phrase","title":"Text-based Detection"},{"location":"references/predictions/image-orientation/","text":"Image Orientation Detection The image orientation feature automatically detects and corrects incorrectly oriented selected product images (front, ingredients, nutrition, and packaging) using text orientation analysis. How it works Detection : Google Cloud Vision OCR analyzes text orientation in uploaded product images. Insight Generation : When text is detected with non-upright orientation (\u226595% confidence) in a selected image, an image_orientation insight is generated. Rotation Application : When the insight is accepted, or if the insight was marked as automatically processable, the selected image is rotated to the correct orientation via the Product Opener API, while preserving any crop bounding boxes. Technical Implementation Detection Algorithm The detection uses OCR orientation metadata to identify incorrectly oriented images. The algorithm: Counts text blocks in each orientation (up, left, right, down). Calculates the confidence as the percentage of text in the dominant non-upright orientation. Determines the required rotation angle (0, 90, 180, or 270 degrees). Only images with \u226595% confidence in a consistent non-upright orientation generate insights. If more than 10 words are detected on the image, we mark the insight as automatically processable , which means the rotation will be automatically applied without user confirmation. This 10-word threshold is a heuristic is based on manual inspection of the generated insights: almost all false positives were generated from images with less than 10 words. Importer The ImageOrientationImporter class: Validates that images are selected (front, nutrition, ingredients, packaging). Ensures the confidence threshold (\u226595%) is met. Checks that the current rotation angle differs from the predicted one. Creates insights with appropriate rotation information. Annotator The ImageOrientationAnnotator class: Processes the rotation annotation. Transforms any existing crop bounding boxes to account for the rotation. Calls the Product Opener API to apply the rotation. Bounding Box Transformation When rotating images with existing crop bounding boxes, the coordinates are transformed using matrix rotation to maintain the correct cropping area in the new orientation: The original coordinates are converted to a standard format. A rotation transformation is applied based on the rotation angle. The transformed coordinates are adjusted to fit within image boundaries. Example For a sideways nutrition image: OCR analysis detects text oriented to the right. An insight with rotation: 270 is generated. When applied, the image is rotated 270\u00b0 counter-clockwise. Any crop bounding box is automatically transformed to match the new orientation. Limitations Requires clear, readable text for reliable orientation detection. Images with mixed text orientations may not reach the confidence threshold. Rotation applies to the entire selected image, affecting all cropping areas.","title":"Image Orientation Detection"},{"location":"references/predictions/image-orientation/#image-orientation-detection","text":"The image orientation feature automatically detects and corrects incorrectly oriented selected product images (front, ingredients, nutrition, and packaging) using text orientation analysis.","title":"Image Orientation Detection"},{"location":"references/predictions/image-orientation/#how-it-works","text":"Detection : Google Cloud Vision OCR analyzes text orientation in uploaded product images. Insight Generation : When text is detected with non-upright orientation (\u226595% confidence) in a selected image, an image_orientation insight is generated. Rotation Application : When the insight is accepted, or if the insight was marked as automatically processable, the selected image is rotated to the correct orientation via the Product Opener API, while preserving any crop bounding boxes.","title":"How it works"},{"location":"references/predictions/image-orientation/#technical-implementation","text":"","title":"Technical Implementation"},{"location":"references/predictions/image-orientation/#detection-algorithm","text":"The detection uses OCR orientation metadata to identify incorrectly oriented images. The algorithm: Counts text blocks in each orientation (up, left, right, down). Calculates the confidence as the percentage of text in the dominant non-upright orientation. Determines the required rotation angle (0, 90, 180, or 270 degrees). Only images with \u226595% confidence in a consistent non-upright orientation generate insights. If more than 10 words are detected on the image, we mark the insight as automatically processable , which means the rotation will be automatically applied without user confirmation. This 10-word threshold is a heuristic is based on manual inspection of the generated insights: almost all false positives were generated from images with less than 10 words.","title":"Detection Algorithm"},{"location":"references/predictions/image-orientation/#importer","text":"The ImageOrientationImporter class: Validates that images are selected (front, nutrition, ingredients, packaging). Ensures the confidence threshold (\u226595%) is met. Checks that the current rotation angle differs from the predicted one. Creates insights with appropriate rotation information.","title":"Importer"},{"location":"references/predictions/image-orientation/#annotator","text":"The ImageOrientationAnnotator class: Processes the rotation annotation. Transforms any existing crop bounding boxes to account for the rotation. Calls the Product Opener API to apply the rotation.","title":"Annotator"},{"location":"references/predictions/image-orientation/#bounding-box-transformation","text":"When rotating images with existing crop bounding boxes, the coordinates are transformed using matrix rotation to maintain the correct cropping area in the new orientation: The original coordinates are converted to a standard format. A rotation transformation is applied based on the rotation angle. The transformed coordinates are adjusted to fit within image boundaries.","title":"Bounding Box Transformation"},{"location":"references/predictions/image-orientation/#example","text":"For a sideways nutrition image: OCR analysis detects text oriented to the right. An insight with rotation: 270 is generated. When applied, the image is rotated 270\u00b0 counter-clockwise. Any crop bounding box is automatically transformed to match the new orientation.","title":"Example"},{"location":"references/predictions/image-orientation/#limitations","text":"Requires clear, readable text for reliable orientation detection. Images with mixed text orientations may not reach the confidence threshold. Rotation applies to the entire selected image, affecting all cropping areas.","title":"Limitations"},{"location":"references/predictions/ingredient-detection/","text":"Ingredient detection Dataset on Hugging Face - Model on Hugging Face - Training notes We developped a sequence tagging model to automatically extract ingredient lists from photos of product packaging. This model solely relies on the text content of the image, and does not require any spatial information. It is trained to detect ingredient lists in a sequence tagging format, where each token (word) is classified as either part of an ingredient list or not. The model uses the IOB format for entity classes. There is a single \u00ccNG entity class for ingredient lists, and the O class for tokens that are not part of an ingredient list. The model was fine-tuned from xlm-roberta-large model . Dataset The model was trained on ~5000 texts extracted from product packaging, that were annotated semi-automatically. Look at the dataset page on Hugging Face for more details. Robotoff integration Inference and post-processing The model was exported to ONNX and is served by Triton server. The model integration in Robotoff can be found in the robotoff.prediction.ingredient_list module. After the individual token prediction, as often we aggregate the entities by merging the tokens that have the same entity class. We detect the language associated by the ingredient list using our language detection model (currently based on fasttext), and parse the ingredient list using the Product Opener ingredient parser. This allows us to know how many ingredients in the ingredient list are recognized (or not) by Open Food Facts. We add 7 fields in the image prediction data field: lang : the prediction of the language identification model - lang : the ISO 639-1 code of the language detected in the ingredient list (ex: fr for French, en for English, de for German, etc.) - confidence : the confidence of the model ingredients_n : the number of ingredients in the ingredient list known_ingredients_n : the number of ingredients in the ingredient list that are recognized by Open Food Facts unknown_ingredients_n : the number of ingredients in the ingredient list that are not recognized by Open Food Facts fraction_known_ingredients : the fraction of ingredients in the ingredient list that are recognized by Open Food Facts (between 0 and 1) ingredients : The list of parsed ingredients, as a list of dictionaries with the following fields: id : the canonical ingredient ID in Open Food Facts text : the ingredient text \u00ecn_taxonomy : True if the ingredient is in the Open Food Facts taxonomy, False otherwise other fields returned by the Product Opener ingredient parser API such as vegan , vegetarian , percent_max , percent_min , percent , percent_estimate , ciqual_food_code ,... bounding_box : the bounding box of the ingredient list in the image in relative coordinates, as a list of 4 float values (x_min, y_min, x_max, y_max). Integration For every new uploaded image, the model is run on this image 1 . As for all computer vision models, we save the model prediction in the image_prediction table. If some entities (i.e. ingredient lists) are detected, we create a Prediction in DB using the usual import mechanism 2 , under the type ingredient_list . We only create an insight if the following conditions are met 3 : there is no ingredient list for the language detected in the extracted ingredient list 60% or more of the ingredients in the extracted ingredient list are recognized by Open Food Facts as ingredients (using the Product Opener parser) The insight has a null value and the detected language as value_tag . An example of the data field is given below: { \"entities\" : [ { \"end\" : 455 , \"lang\" : { \"lang\" : \"fr\" , \"confidence\" : 0.69290453 }, \"text\" : \"entier partiellement concentr\u00e9 pasteuris\u00e9 81% (origine: France), eau, sucre 6,9%, miel 1,6%, Poids net biscuits poudre 0,66% (farine de bl\u00e9, sucre, beurre, sel), amidon de ma\u00efs, jus de citron concentr\u00e9, cannelle 0,16%, badiane 280a 0,16%, ar\u00f4me naturel de cannelle 0,16%, ferments lactiques (dont lait). Peut contenir des traces d'oeuf\" , \"score\" : 0.9999919533729553 , \"start\" : 120 , \"raw_end\" : 423 , \"ingredients\" : [ { \"id\" : \"fr:entier-partiellement-concentre-pasteurise\" , \"text\" : \"entier partiellement concentr\u00e9 pasteuris\u00e9\" , \"origins\" : \"en:france\" , \"percent\" : 81 , \"in_taxonomy\" : false , \"percent_max\" : 81 , \"percent_min\" : 81 , \"percent_estimate\" : 81 }, { \"id\" : \"en:water\" , \"text\" : \"eau\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 9.04 , \"percent_min\" : 7.88 , \"ciqual_food_code\" : \"18066\" , \"percent_estimate\" : 8.46 }, { \"id\" : \"en:sugar\" , \"text\" : \"sucre\" , \"vegan\" : \"yes\" , \"percent\" : 6.9 , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 6.9 , \"percent_min\" : 6.9 , \"percent_estimate\" : 6.9 }, { \"id\" : \"en:honey\" , \"text\" : \"miel\" , \"vegan\" : \"no\" , \"percent\" : 1.6 , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 1.6 , \"percent_min\" : 1.6 , \"ciqual_food_code\" : \"31008\" , \"percent_estimate\" : 1.6 }, { \"id\" : \"fr:poids-net-biscuits-poudre\" , \"text\" : \"Poids net biscuits poudre\" , \"percent\" : 0.66 , \"in_taxonomy\" : false , \"ingredients\" : [ { \"id\" : \"en:wheat-flour\" , \"text\" : \"farine de bl\u00e9\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.66 , \"percent_min\" : 0.165 , \"percent_estimate\" : 0.4125 }, { \"id\" : \"en:sugar\" , \"text\" : \"sucre\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.33 , \"percent_min\" : 0 , \"percent_estimate\" : 0.12375 }, { \"id\" : \"en:butter\" , \"text\" : \"beurre\" , \"vegan\" : \"no\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.22 , \"percent_min\" : 0 , \"percent_estimate\" : 0.061875 }, { \"id\" : \"en:salt\" , \"text\" : \"sel\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.22 , \"percent_min\" : 0 , \"ciqual_food_code\" : \"11058\" , \"percent_estimate\" : 0.061875 } ], \"percent_max\" : 0.66 , \"percent_min\" : 0.66 , \"percent_estimate\" : 0.66 }, { \"id\" : \"en:corn-starch\" , \"text\" : \"amidon de ma\u00efs\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.66 , \"percent_min\" : 0.16 , \"ciqual_food_code\" : \"9510\" , \"percent_estimate\" : 0.41 }, { \"id\" : \"en:concentrated-lemon-juice\" , \"text\" : \"jus de citron concentr\u00e9\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.66 , \"percent_min\" : 0.16 , \"ciqual_food_code\" : \"2028\" , \"percent_estimate\" : 0.41 }, { \"id\" : \"en:cinnamon\" , \"text\" : \"cannelle\" , \"vegan\" : \"yes\" , \"percent\" : 0.16 , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.16 , \"percent_min\" : 0.16 , \"percent_estimate\" : 0.16 }, { \"id\" : \"fr:badiane-280a\" , \"text\" : \"badiane 280a\" , \"percent\" : 0.16 , \"in_taxonomy\" : false , \"percent_max\" : 0.16 , \"percent_min\" : 0.16 , \"percent_estimate\" : 0.16 }, { \"id\" : \"en:natural-cinammon-flavouring\" , \"text\" : \"ar\u00f4me naturel de cannelle\" , \"vegan\" : \"maybe\" , \"percent\" : 0.16 , \"vegetarian\" : \"maybe\" , \"in_taxonomy\" : true , \"percent_max\" : 0.16 , \"percent_min\" : 0.16 , \"percent_estimate\" : 0.16 }, { \"id\" : \"en:lactic-ferments\" , \"text\" : \"ferments lactiques\" , \"vegan\" : \"maybe\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"ingredients\" : [ { \"id\" : \"en:milk\" , \"text\" : \"dont lait\" , \"vegan\" : \"no\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.16 , \"percent_min\" : 0 , \"percent_estimate\" : 0.0800000000000267 } ], \"percent_max\" : 0.16 , \"percent_min\" : 0 , \"percent_estimate\" : 0.0800000000000267 } ], \"bounding_box\" : [ 1502 , 237 , 1839 , 3831 ], \"ingredients_n\" : 16 , \"known_ingredients_n\" : 13 , \"unknown_ingredients_n\" : 3 } ] } Annotation To annotate the insight, use the usual route POST https://robotoff.openfoodfacts.org/api/v1/insights/annotate . The request body depends on whether the user validates the ingredient list as-it or updates it. If the user updated the ingredient list (for example if a typo was present), pass annotation=2 and as data a JSON object serialized as a string with the annotation field set to the updated ingredient list. If the user validates the ingredient list as-it, pass annotation=1 and don't pass the data field. Finally, the user rejects the prediction, pass annotation=0 and don't pass the data field. If the insight is validated, we select the source image as an ingredient image for the detected language (if needed), with the right cropping and image orientation. If the user submitted an updated ingredient list, we select the image without cropping as we don't know the correct crop associated with the updated ingredient list. See function robotoff.workers.tasks.import_image.extract_ingredients_job \u21a9 See this page for more details \u21a9 See IngredientDetectionImporter.generate_candidates for implementation \u21a9","title":"Ingredient detection"},{"location":"references/predictions/ingredient-detection/#ingredient-detection","text":"Dataset on Hugging Face - Model on Hugging Face - Training notes We developped a sequence tagging model to automatically extract ingredient lists from photos of product packaging. This model solely relies on the text content of the image, and does not require any spatial information. It is trained to detect ingredient lists in a sequence tagging format, where each token (word) is classified as either part of an ingredient list or not. The model uses the IOB format for entity classes. There is a single \u00ccNG entity class for ingredient lists, and the O class for tokens that are not part of an ingredient list. The model was fine-tuned from xlm-roberta-large model .","title":"Ingredient detection"},{"location":"references/predictions/ingredient-detection/#dataset","text":"The model was trained on ~5000 texts extracted from product packaging, that were annotated semi-automatically. Look at the dataset page on Hugging Face for more details.","title":"Dataset"},{"location":"references/predictions/ingredient-detection/#robotoff-integration","text":"","title":"Robotoff integration"},{"location":"references/predictions/ingredient-detection/#inference-and-post-processing","text":"The model was exported to ONNX and is served by Triton server. The model integration in Robotoff can be found in the robotoff.prediction.ingredient_list module. After the individual token prediction, as often we aggregate the entities by merging the tokens that have the same entity class. We detect the language associated by the ingredient list using our language detection model (currently based on fasttext), and parse the ingredient list using the Product Opener ingredient parser. This allows us to know how many ingredients in the ingredient list are recognized (or not) by Open Food Facts. We add 7 fields in the image prediction data field: lang : the prediction of the language identification model - lang : the ISO 639-1 code of the language detected in the ingredient list (ex: fr for French, en for English, de for German, etc.) - confidence : the confidence of the model ingredients_n : the number of ingredients in the ingredient list known_ingredients_n : the number of ingredients in the ingredient list that are recognized by Open Food Facts unknown_ingredients_n : the number of ingredients in the ingredient list that are not recognized by Open Food Facts fraction_known_ingredients : the fraction of ingredients in the ingredient list that are recognized by Open Food Facts (between 0 and 1) ingredients : The list of parsed ingredients, as a list of dictionaries with the following fields: id : the canonical ingredient ID in Open Food Facts text : the ingredient text \u00ecn_taxonomy : True if the ingredient is in the Open Food Facts taxonomy, False otherwise other fields returned by the Product Opener ingredient parser API such as vegan , vegetarian , percent_max , percent_min , percent , percent_estimate , ciqual_food_code ,... bounding_box : the bounding box of the ingredient list in the image in relative coordinates, as a list of 4 float values (x_min, y_min, x_max, y_max).","title":"Inference and post-processing"},{"location":"references/predictions/ingredient-detection/#integration","text":"For every new uploaded image, the model is run on this image 1 . As for all computer vision models, we save the model prediction in the image_prediction table. If some entities (i.e. ingredient lists) are detected, we create a Prediction in DB using the usual import mechanism 2 , under the type ingredient_list . We only create an insight if the following conditions are met 3 : there is no ingredient list for the language detected in the extracted ingredient list 60% or more of the ingredients in the extracted ingredient list are recognized by Open Food Facts as ingredients (using the Product Opener parser) The insight has a null value and the detected language as value_tag . An example of the data field is given below: { \"entities\" : [ { \"end\" : 455 , \"lang\" : { \"lang\" : \"fr\" , \"confidence\" : 0.69290453 }, \"text\" : \"entier partiellement concentr\u00e9 pasteuris\u00e9 81% (origine: France), eau, sucre 6,9%, miel 1,6%, Poids net biscuits poudre 0,66% (farine de bl\u00e9, sucre, beurre, sel), amidon de ma\u00efs, jus de citron concentr\u00e9, cannelle 0,16%, badiane 280a 0,16%, ar\u00f4me naturel de cannelle 0,16%, ferments lactiques (dont lait). Peut contenir des traces d'oeuf\" , \"score\" : 0.9999919533729553 , \"start\" : 120 , \"raw_end\" : 423 , \"ingredients\" : [ { \"id\" : \"fr:entier-partiellement-concentre-pasteurise\" , \"text\" : \"entier partiellement concentr\u00e9 pasteuris\u00e9\" , \"origins\" : \"en:france\" , \"percent\" : 81 , \"in_taxonomy\" : false , \"percent_max\" : 81 , \"percent_min\" : 81 , \"percent_estimate\" : 81 }, { \"id\" : \"en:water\" , \"text\" : \"eau\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 9.04 , \"percent_min\" : 7.88 , \"ciqual_food_code\" : \"18066\" , \"percent_estimate\" : 8.46 }, { \"id\" : \"en:sugar\" , \"text\" : \"sucre\" , \"vegan\" : \"yes\" , \"percent\" : 6.9 , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 6.9 , \"percent_min\" : 6.9 , \"percent_estimate\" : 6.9 }, { \"id\" : \"en:honey\" , \"text\" : \"miel\" , \"vegan\" : \"no\" , \"percent\" : 1.6 , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 1.6 , \"percent_min\" : 1.6 , \"ciqual_food_code\" : \"31008\" , \"percent_estimate\" : 1.6 }, { \"id\" : \"fr:poids-net-biscuits-poudre\" , \"text\" : \"Poids net biscuits poudre\" , \"percent\" : 0.66 , \"in_taxonomy\" : false , \"ingredients\" : [ { \"id\" : \"en:wheat-flour\" , \"text\" : \"farine de bl\u00e9\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.66 , \"percent_min\" : 0.165 , \"percent_estimate\" : 0.4125 }, { \"id\" : \"en:sugar\" , \"text\" : \"sucre\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.33 , \"percent_min\" : 0 , \"percent_estimate\" : 0.12375 }, { \"id\" : \"en:butter\" , \"text\" : \"beurre\" , \"vegan\" : \"no\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.22 , \"percent_min\" : 0 , \"percent_estimate\" : 0.061875 }, { \"id\" : \"en:salt\" , \"text\" : \"sel\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.22 , \"percent_min\" : 0 , \"ciqual_food_code\" : \"11058\" , \"percent_estimate\" : 0.061875 } ], \"percent_max\" : 0.66 , \"percent_min\" : 0.66 , \"percent_estimate\" : 0.66 }, { \"id\" : \"en:corn-starch\" , \"text\" : \"amidon de ma\u00efs\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.66 , \"percent_min\" : 0.16 , \"ciqual_food_code\" : \"9510\" , \"percent_estimate\" : 0.41 }, { \"id\" : \"en:concentrated-lemon-juice\" , \"text\" : \"jus de citron concentr\u00e9\" , \"vegan\" : \"yes\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.66 , \"percent_min\" : 0.16 , \"ciqual_food_code\" : \"2028\" , \"percent_estimate\" : 0.41 }, { \"id\" : \"en:cinnamon\" , \"text\" : \"cannelle\" , \"vegan\" : \"yes\" , \"percent\" : 0.16 , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.16 , \"percent_min\" : 0.16 , \"percent_estimate\" : 0.16 }, { \"id\" : \"fr:badiane-280a\" , \"text\" : \"badiane 280a\" , \"percent\" : 0.16 , \"in_taxonomy\" : false , \"percent_max\" : 0.16 , \"percent_min\" : 0.16 , \"percent_estimate\" : 0.16 }, { \"id\" : \"en:natural-cinammon-flavouring\" , \"text\" : \"ar\u00f4me naturel de cannelle\" , \"vegan\" : \"maybe\" , \"percent\" : 0.16 , \"vegetarian\" : \"maybe\" , \"in_taxonomy\" : true , \"percent_max\" : 0.16 , \"percent_min\" : 0.16 , \"percent_estimate\" : 0.16 }, { \"id\" : \"en:lactic-ferments\" , \"text\" : \"ferments lactiques\" , \"vegan\" : \"maybe\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"ingredients\" : [ { \"id\" : \"en:milk\" , \"text\" : \"dont lait\" , \"vegan\" : \"no\" , \"vegetarian\" : \"yes\" , \"in_taxonomy\" : true , \"percent_max\" : 0.16 , \"percent_min\" : 0 , \"percent_estimate\" : 0.0800000000000267 } ], \"percent_max\" : 0.16 , \"percent_min\" : 0 , \"percent_estimate\" : 0.0800000000000267 } ], \"bounding_box\" : [ 1502 , 237 , 1839 , 3831 ], \"ingredients_n\" : 16 , \"known_ingredients_n\" : 13 , \"unknown_ingredients_n\" : 3 } ] }","title":"Integration"},{"location":"references/predictions/ingredient-detection/#annotation","text":"To annotate the insight, use the usual route POST https://robotoff.openfoodfacts.org/api/v1/insights/annotate . The request body depends on whether the user validates the ingredient list as-it or updates it. If the user updated the ingredient list (for example if a typo was present), pass annotation=2 and as data a JSON object serialized as a string with the annotation field set to the updated ingredient list. If the user validates the ingredient list as-it, pass annotation=1 and don't pass the data field. Finally, the user rejects the prediction, pass annotation=0 and don't pass the data field. If the insight is validated, we select the source image as an ingredient image for the detected language (if needed), with the right cropping and image orientation. If the user submitted an updated ingredient list, we select the image without cropping as we don't know the correct crop associated with the updated ingredient list. See function robotoff.workers.tasks.import_image.extract_ingredients_job \u21a9 See this page for more details \u21a9 See IngredientDetectionImporter.generate_candidates for implementation \u21a9","title":"Annotation"},{"location":"references/predictions/nutrient-extraction/","text":"Nutrition extraction Dataset on Hugging Face - Model on Hugging Face - Are you looking to integrate it in your app? We developped a ML model to automatically extract nutrition information from photos of product packaging where nutrition facts are displayed. This model detects the most common nutrition values (proteins, salt, energy-kj,...), either for 100g or per serving. We use LayoutLMv3, an architecture used in Document AI to perform various tasks on structured documents (bills, receipts, reports,...). The model expects the input image, the tokens (=words) and the spatial position of each token on the image. As the model requires token text and position as input, an OCR must be performed beforehand. We use Google Cloud Vision to extract text content from the image. LayoutLMv3 architecture can perform several tasks, we frame the problem as a token classification task. The model must predict the class of each token, among a predefined set of classes. We follow the IOB format for entity classes. Here is a complete list of the token classes detected by the model: O B-ENERGY_KJ_SERVING I-ENERGY_KJ_SERVING B-CARBOHYDRATES_100G I-CARBOHYDRATES_100G B-CHOLESTEROL_SERVING I-CHOLESTEROL_SERVING B-ENERGY_KCAL_100G I-ENERGY_KCAL_100G B-SALT_SERVING I-SALT_SERVING B-SALT_100G I-SALT_100G B-SERVING_SIZE I-SERVING_SIZE B-CALCIUM_100G I-CALCIUM_100G B-SODIUM_SERVING I-SODIUM_SERVING B-FIBER_100G I-FIBER_100G B-IRON_SERVING I-IRON_SERVING B-IRON_100G I-IRON_100G B-POTASSIUM_100G I-POTASSIUM_100G B-CALCIUM_SERVING I-CALCIUM_SERVING B-TRANS_FAT_100G I-TRANS_FAT_100G B-SATURATED_FAT_100G I-SATURATED_FAT_100G B-PROTEINS_SERVING I-PROTEINS_SERVING B-SATURATED_FAT_SERVING I-SATURATED_FAT_SERVING B-VITAMIN_D_100G I-VITAMIN_D_100G B-ENERGY_KJ_100G I-ENERGY_KJ_100G B-FAT_100G I-FAT_100G B-PROTEINS_100G I-PROTEINS_100G B-VITAMIN_D_SERVING I-VITAMIN_D_SERVING B-ADDED_SUGARS_SERVING I-ADDED_SUGARS_SERVING B-CHOLESTEROL_100G I-CHOLESTEROL_100G B-SUGARS_100G I-SUGARS_100G B-CARBOHYDRATES_SERVING I-CARBOHYDRATES_SERVING B-ADDED_SUGARS_100G I-ADDED_SUGARS_100G B-SODIUM_100G I-SODIUM_100G B-FIBER_SERVING I-FIBER_SERVING B-SUGARS_SERVING I-SUGARS_SERVING B-ENERGY_KCAL_SERVING I-ENERGY_KCAL_SERVING B-FAT_SERVING I-FAT_SERVING B-TRANS_FAT_SERVING I-TRANS_FAT_SERVING B-POTASSIUM_SERVING I-POTASSIUM_SERVING Nutrients that are not in this list are detected as O 1 . Dataset Random images selected as nutrition images were picked for annotation. Using the list of labels above, more than 3500 images were manually annotated. To learn more about the dataset, have a look at the description of the dataset on Hugging Face . Robotoff integration Pre-processing, inference and post-processing The model was exported to ONNX and is served by Triton server. The model integration in Robotoff can be found in robotoff.prediction.nutrition_extraction module. The predict function 2 takes as input the image (as a Pillow Image) and the Google Cloud Vision OCR result (as a OCRResult object). When extracting nutrient information from an image, we perform the following steps: extract the words and their coordinates from the OCR result preprocess the image, the words and their coordinates using the LayoutLMv3 preprocessor, that takes care of preprocessing the data in the right format for the LayoutLMv3 model perform the inference: the request is sent to Triton server through gRPC postprocess the results Postprocessing includes the following steps: gather pre-entities from individual labels. There is one pre-entities for each input token. aggregate entities: the 'O' (OTHER) entity is ignored, and pre-entities with the same entity class are merged together. post-process entities: we post-process the detected text to correct some known limitations of the model, and we extract the value (ex: 5 ) and the unit (ex: g ) from the entity text. The predict function returns a NutritionExtractionPrediction dataclass that has two fields: nutrients contains postprocessed entities that were considered valid during post-processing (the valid field described below is therefore not present). entities contains the raw pre-entities, the aggregated entities and the post-processed entities (respectively in the raw , aggregated and postprocessed fields). This field is useful for debugging and understanding model predictions. Postprocessed entities contain the following fields: entity : the nutrient name, in Product Opener format (ex: energy-kcal_100g or salt_serving ) text : the text of the entity (ex: 125 kJ ) value : the nutrient value. It's either a number or traces unit : the nutrient unit, either g , mg , \u00b5g , kj , kcal or null . Sometimes the nutrient unit is not present after the value, or the OCR didn't detect the corresponding word. You can either infer a plausible unit given the entity (ex: g for proteins, carbohydrates,...) or ignore this entity. score : The entity score. We use the score of the first pre-entity as the aggregated entity score. start : the word start index of the entity, with respect to the original OCR JSON end : the word end index of the entity, with respect to the original OCR JSON char_start : the character start index of the entity, with respect to the original OCR JSON char_end : the character end index of the entity, with respect to the original OCR JSON valid : whether the extracted entity is valid. We consider an entity invalid if we couldn't extract nutrient value from the text field, or if there are more than one entity for a single nutrient. For example, two proteins_100g entities are both considered invalid, but one proteins_100g and one proteins_serving are considered valid. Integration For every new uploaded image, the model is run on this image 3 . As for all computer vision models, we save the model prediction in the image_prediction table. If some entities were detected, we create a Prediction in DB using the usual import mechanism 4 , under the type nutrient_extraction . We only create an insight if the model detected a nutrient whose value is different than what's registered on the product (this includes nutrients with missing value) 5 . We only consider nutrient values entered for the same quantity (100g or serving) as what's indicated in Open Food Facts. It means that for a product with nutrition_data_per=serving and missing proteins_serving , we won't generate an insight if the model detected the protein quantity per 100g ( proteins_100g ). We consider images from the most recent image to the oldest one, and stop the process as soon as a valid candidate insight is found. This ensures that the nutrient prediction is made on the most recent image available. Indeed, nutrition values can change over time, and we want to avoid creating insights based on possibly outdated images. If a nutrient_extraction insight was already validated for a product, we only create a new insight if the associated image is more recent than the image that was used to create the last validated insight. Using a fixed set of classes is not the best approach when we have many classes. It however allows us to use LayoutLM architecture, which is very performant for this task, even when the nutrition table is hard to read due to packaging deformations or alterations. To detect the long-tail of nutrients, approaches using graph-based approach, where we would map a nutrient mention to its value, could be explored in the future. \u21a9 In robotoff.prediction.nutrition_extraction module \u21a9 See function robotoff.workers.tasks.import_image.extract_nutrition_job \u21a9 See this page for more details \u21a9 See NutrientExtractionImporter.generate_candidates for implementation \u21a9","title":"Nutrition extraction"},{"location":"references/predictions/nutrient-extraction/#nutrition-extraction","text":"Dataset on Hugging Face - Model on Hugging Face - Are you looking to integrate it in your app? We developped a ML model to automatically extract nutrition information from photos of product packaging where nutrition facts are displayed. This model detects the most common nutrition values (proteins, salt, energy-kj,...), either for 100g or per serving. We use LayoutLMv3, an architecture used in Document AI to perform various tasks on structured documents (bills, receipts, reports,...). The model expects the input image, the tokens (=words) and the spatial position of each token on the image. As the model requires token text and position as input, an OCR must be performed beforehand. We use Google Cloud Vision to extract text content from the image. LayoutLMv3 architecture can perform several tasks, we frame the problem as a token classification task. The model must predict the class of each token, among a predefined set of classes. We follow the IOB format for entity classes. Here is a complete list of the token classes detected by the model: O B-ENERGY_KJ_SERVING I-ENERGY_KJ_SERVING B-CARBOHYDRATES_100G I-CARBOHYDRATES_100G B-CHOLESTEROL_SERVING I-CHOLESTEROL_SERVING B-ENERGY_KCAL_100G I-ENERGY_KCAL_100G B-SALT_SERVING I-SALT_SERVING B-SALT_100G I-SALT_100G B-SERVING_SIZE I-SERVING_SIZE B-CALCIUM_100G I-CALCIUM_100G B-SODIUM_SERVING I-SODIUM_SERVING B-FIBER_100G I-FIBER_100G B-IRON_SERVING I-IRON_SERVING B-IRON_100G I-IRON_100G B-POTASSIUM_100G I-POTASSIUM_100G B-CALCIUM_SERVING I-CALCIUM_SERVING B-TRANS_FAT_100G I-TRANS_FAT_100G B-SATURATED_FAT_100G I-SATURATED_FAT_100G B-PROTEINS_SERVING I-PROTEINS_SERVING B-SATURATED_FAT_SERVING I-SATURATED_FAT_SERVING B-VITAMIN_D_100G I-VITAMIN_D_100G B-ENERGY_KJ_100G I-ENERGY_KJ_100G B-FAT_100G I-FAT_100G B-PROTEINS_100G I-PROTEINS_100G B-VITAMIN_D_SERVING I-VITAMIN_D_SERVING B-ADDED_SUGARS_SERVING I-ADDED_SUGARS_SERVING B-CHOLESTEROL_100G I-CHOLESTEROL_100G B-SUGARS_100G I-SUGARS_100G B-CARBOHYDRATES_SERVING I-CARBOHYDRATES_SERVING B-ADDED_SUGARS_100G I-ADDED_SUGARS_100G B-SODIUM_100G I-SODIUM_100G B-FIBER_SERVING I-FIBER_SERVING B-SUGARS_SERVING I-SUGARS_SERVING B-ENERGY_KCAL_SERVING I-ENERGY_KCAL_SERVING B-FAT_SERVING I-FAT_SERVING B-TRANS_FAT_SERVING I-TRANS_FAT_SERVING B-POTASSIUM_SERVING I-POTASSIUM_SERVING Nutrients that are not in this list are detected as O 1 .","title":"Nutrition extraction"},{"location":"references/predictions/nutrient-extraction/#dataset","text":"Random images selected as nutrition images were picked for annotation. Using the list of labels above, more than 3500 images were manually annotated. To learn more about the dataset, have a look at the description of the dataset on Hugging Face .","title":"Dataset"},{"location":"references/predictions/nutrient-extraction/#robotoff-integration","text":"","title":"Robotoff integration"},{"location":"references/predictions/nutrient-extraction/#pre-processing-inference-and-post-processing","text":"The model was exported to ONNX and is served by Triton server. The model integration in Robotoff can be found in robotoff.prediction.nutrition_extraction module. The predict function 2 takes as input the image (as a Pillow Image) and the Google Cloud Vision OCR result (as a OCRResult object). When extracting nutrient information from an image, we perform the following steps: extract the words and their coordinates from the OCR result preprocess the image, the words and their coordinates using the LayoutLMv3 preprocessor, that takes care of preprocessing the data in the right format for the LayoutLMv3 model perform the inference: the request is sent to Triton server through gRPC postprocess the results Postprocessing includes the following steps: gather pre-entities from individual labels. There is one pre-entities for each input token. aggregate entities: the 'O' (OTHER) entity is ignored, and pre-entities with the same entity class are merged together. post-process entities: we post-process the detected text to correct some known limitations of the model, and we extract the value (ex: 5 ) and the unit (ex: g ) from the entity text. The predict function returns a NutritionExtractionPrediction dataclass that has two fields: nutrients contains postprocessed entities that were considered valid during post-processing (the valid field described below is therefore not present). entities contains the raw pre-entities, the aggregated entities and the post-processed entities (respectively in the raw , aggregated and postprocessed fields). This field is useful for debugging and understanding model predictions. Postprocessed entities contain the following fields: entity : the nutrient name, in Product Opener format (ex: energy-kcal_100g or salt_serving ) text : the text of the entity (ex: 125 kJ ) value : the nutrient value. It's either a number or traces unit : the nutrient unit, either g , mg , \u00b5g , kj , kcal or null . Sometimes the nutrient unit is not present after the value, or the OCR didn't detect the corresponding word. You can either infer a plausible unit given the entity (ex: g for proteins, carbohydrates,...) or ignore this entity. score : The entity score. We use the score of the first pre-entity as the aggregated entity score. start : the word start index of the entity, with respect to the original OCR JSON end : the word end index of the entity, with respect to the original OCR JSON char_start : the character start index of the entity, with respect to the original OCR JSON char_end : the character end index of the entity, with respect to the original OCR JSON valid : whether the extracted entity is valid. We consider an entity invalid if we couldn't extract nutrient value from the text field, or if there are more than one entity for a single nutrient. For example, two proteins_100g entities are both considered invalid, but one proteins_100g and one proteins_serving are considered valid.","title":"Pre-processing, inference and post-processing"},{"location":"references/predictions/nutrient-extraction/#integration","text":"For every new uploaded image, the model is run on this image 3 . As for all computer vision models, we save the model prediction in the image_prediction table. If some entities were detected, we create a Prediction in DB using the usual import mechanism 4 , under the type nutrient_extraction . We only create an insight if the model detected a nutrient whose value is different than what's registered on the product (this includes nutrients with missing value) 5 . We only consider nutrient values entered for the same quantity (100g or serving) as what's indicated in Open Food Facts. It means that for a product with nutrition_data_per=serving and missing proteins_serving , we won't generate an insight if the model detected the protein quantity per 100g ( proteins_100g ). We consider images from the most recent image to the oldest one, and stop the process as soon as a valid candidate insight is found. This ensures that the nutrient prediction is made on the most recent image available. Indeed, nutrition values can change over time, and we want to avoid creating insights based on possibly outdated images. If a nutrient_extraction insight was already validated for a product, we only create a new insight if the associated image is more recent than the image that was used to create the last validated insight. Using a fixed set of classes is not the best approach when we have many classes. It however allows us to use LayoutLM architecture, which is very performant for this task, even when the nutrition table is hard to read due to packaging deformations or alterations. To detect the long-tail of nutrients, approaches using graph-based approach, where we would map a nutrient mention to its value, could be explored in the future. \u21a9 In robotoff.prediction.nutrition_extraction module \u21a9 See function robotoff.workers.tasks.import_image.extract_nutrition_job \u21a9 See this page for more details \u21a9 See NutrientExtractionImporter.generate_candidates for implementation \u21a9","title":"Integration"},{"location":"references/predictions/nutrition-table/","text":"Nutrition photo selection Every product should have a nutrition photo selected if nutrition facts are visible on the packaging. For multilingual products, we only want a nutrition table to be selected for the main language of the product to avoid unnecessary image duplication, except in the rare cases where we have distinct table for different languages. We detect nutrition tables using a mix of string matching ( regex ) 1 and machine learning detections. We use nutrient_mention insights to fetch all nutrient mentions, in all supported languages: nutrient names (\"sugar\", \"carbohydrates\", \"nutritional information\", \"energy\",...) nutrient values We also use nutrient insights 2 , that detect nutrient name and values that are consecutive in the OCR string, to assign a higher priority to images that also nutrient insights in addition to nutrient_mention insights ( priority=1 instead of priority=2 ). The detected nutrient names are associated with one or more language (ex: if we detect 'energie', it may be in French, German or Dutch). We check for each image and each detected language if the following rules applies, in which case the image is a candidate for selection as a nutrition table photo 3 : we must have at least 4 nutrient name mentions (\"sugar\", \"energy\",...) in the target language we must have at least 3 nutrient value mentions (\"15 g\", \"252 kJ\",...) we must have at least one energy nutrient value (value ending with \"kJ\" or \"kcal\") the detected language must be the product main language If it exist, we also use the nutrition-table object detector prediction to find a crop of the nutrition table. We only use the cropping information if there is only one nutrition table detected with confidence >=0.9 . If all these conditions apply, we generate an insight. There is maximum one insight generated by product. Note that we generate candidates using the most recent images first (images are sorted by decreasing image IDs), so that the most recent images are considered first: we want the most up-to-date photo possible for nutrition table. see find_nutrient_mentions in robotoff.prediction.ocr.nutrient \u21a9 see find_nutrient_values in robotoff.prediction.ocr.nutrient \u21a9 see NutritionImageImporter.generate_candidates_for_image in robotoff.insights.importer \u21a9","title":"Nutrition photo selection"},{"location":"references/predictions/nutrition-table/#nutrition-photo-selection","text":"Every product should have a nutrition photo selected if nutrition facts are visible on the packaging. For multilingual products, we only want a nutrition table to be selected for the main language of the product to avoid unnecessary image duplication, except in the rare cases where we have distinct table for different languages. We detect nutrition tables using a mix of string matching ( regex ) 1 and machine learning detections. We use nutrient_mention insights to fetch all nutrient mentions, in all supported languages: nutrient names (\"sugar\", \"carbohydrates\", \"nutritional information\", \"energy\",...) nutrient values We also use nutrient insights 2 , that detect nutrient name and values that are consecutive in the OCR string, to assign a higher priority to images that also nutrient insights in addition to nutrient_mention insights ( priority=1 instead of priority=2 ). The detected nutrient names are associated with one or more language (ex: if we detect 'energie', it may be in French, German or Dutch). We check for each image and each detected language if the following rules applies, in which case the image is a candidate for selection as a nutrition table photo 3 : we must have at least 4 nutrient name mentions (\"sugar\", \"energy\",...) in the target language we must have at least 3 nutrient value mentions (\"15 g\", \"252 kJ\",...) we must have at least one energy nutrient value (value ending with \"kJ\" or \"kcal\") the detected language must be the product main language If it exist, we also use the nutrition-table object detector prediction to find a crop of the nutrition table. We only use the cropping information if there is only one nutrition table detected with confidence >=0.9 . If all these conditions apply, we generate an insight. There is maximum one insight generated by product. Note that we generate candidates using the most recent images first (images are sorted by decreasing image IDs), so that the most recent images are considered first: we want the most up-to-date photo possible for nutrition table. see find_nutrient_mentions in robotoff.prediction.ocr.nutrient \u21a9 see find_nutrient_values in robotoff.prediction.ocr.nutrient \u21a9 see NutritionImageImporter.generate_candidates_for_image in robotoff.insights.importer \u21a9","title":"Nutrition photo selection"},{"location":"research/","text":"Overview This page lists the different research projects conducted by the Open Food Facts team and volunteers. Nutrition table detection","title":"Overview"},{"location":"research/#overview","text":"This page lists the different research projects conducted by the Open Food Facts team and volunteers. Nutrition table detection","title":"Overview"},{"location":"research/logo-detection/ann-benchmark/","text":"ANN Benchmark All logos are embedded through a computer vision model. In order to help logos annotation, the nearest neighbors of each logo must be easily accesible. A benchmark has been performed to determine the best way to perform a nearest neighbors search among the following HNSW indexes: (faiss)[https://github.com/facebookresearch/faiss] (redis)[https://redis.io/docs/stack/search/reference/vectors/] (elasticsearch)[https://www.elastic.co/fr/blog/introducing-approximate-nearest-neighbor-search-in-elasticsearch-8-0] For each framework, multiple parameters were used to find the best index in our use case. Our Use Case The data used to test the performance of the indexes was made of 4.371.343 vectors (each one issued from the CLIP vision model with a logo as input) of dimension 768 and type float32. The dimension of each vector used in the benchmark is 768, the size of embeddings CLIP outputs before processing through the projection layer. However, in production, as this last layer of the CLIP model is used, the dimension of the embeddings that are indexed is actually 512. The process to go from 768 to 512 being only a projection, the change of dimensionality should not impact the results. A test of elasticsearch with 512 dimension vectors did indeed lead to very similar results as the ones computed with 768 dimension vectors. The goal is to search for nearest neighbors in real time. The search time must be short. The precision of the search matters as it is important to get logos as close as possible from the query logo for the annotators to be efficient. The feature is expected to work in production on machines that run other tasks in parallel. The memory used by the index and search must thus be as small as possible. Conditions of the tests All HNSW results were compared to FLAT faiss index search (an exact search) results to compute the precision of the index. Only cosine similarity was usedin this benchmark. HNSW indexes are adjustable trhough three parameters: m: the number of connections that each node in the index has to its neighbors efConstruction: the size of the priority queue used during the construction of the index efSearch/efRuntime/num_candidates: the size of the priority queue used during search operations Benchmark You can find the code of the various benchmarks here . Here are the results obtained with Faiss : m efConstruction efSearch micro-recall@1 micro-recall@4 micro-recall@10 micro-recall@50 micro-recall@100 - - - - - - - - 8 128 64 0.582 0.69475 0.706 0.70754 0.6829 8 128 128 0.591 0.70925 0.7238 0.72874 0.72166 8 256 64 0.598 0.714 0.7265 0.72542 0.70022 8 256 128 0.603 0.72225 0.7356 0.74082 0.7346 16 128 64 0.605 0.72875 0.7393 0.74212 0.72658 16 128 128 0.615 0.737 0.7476 0.75206 0.74833 16 256 64 0.623 0.74175 0.7501 0.75244 0.73805 16 256 128 0.624 0.745 0.7546 0.7581 0.75509 The recall never outreaches 0.76. A better recall is wanted. The Redis HNSW indexes were thus explored. Here are the results obtained with Redis : m efConstruction efRuntime micro-recall@1 micro-recall@4 micro-recall@10 micro-recall@50 micro-recall@100 - - - - - - - - 4 128 64 0.799 0.795 0.7968 0.76916 0.72839 4 128 128 0.822 0.81875 0.8215 0.79918 0.76812 4 256 64 0.83 0.8305 0.8254 0.79126 0.74795 4 256 128 0.855 0.8565 0.8511 0.82204 0.78854 8 128 64 0.955 0.94075 0.9318 0.90614 0.88248 8 128 128 0.964 0.95 0.9403 0.91784 0.90007 8 256 64 0.964 0.9515 0.9424 0.91672 0.89339 8 256 128 0.969 0.95725 0.9477 0.92514 0.90855 The efficiency of these indexes, with the right parameters are better. However, Redis loads all the embeddings in RAM when loading the index, which accounts for more than 20 GB of RAM used by it. This is too much for our use case. The last tool explored is ElasticSearch . With the default parameters, here is what was obtained: m efConstruction num_candidates micro-recall@1 micro-recall@4 micro-recall@10 micro-recall@50 micro-recall@100 - - - - - - - - 16 200 50 0.977 0.9635 0.9533 0.93832 0.93512 The results are excellent. The RAM used is bellow 5 GB. And the time search is around 80 ms, which is acceptable for our use case and way bellow the 3 seconds for exact search with a FLAT faiss index. ElasticSearch was thus chosen for computing the ANN search.","title":"ANN Benchmark"},{"location":"research/logo-detection/ann-benchmark/#ann-benchmark","text":"All logos are embedded through a computer vision model. In order to help logos annotation, the nearest neighbors of each logo must be easily accesible. A benchmark has been performed to determine the best way to perform a nearest neighbors search among the following HNSW indexes: (faiss)[https://github.com/facebookresearch/faiss] (redis)[https://redis.io/docs/stack/search/reference/vectors/] (elasticsearch)[https://www.elastic.co/fr/blog/introducing-approximate-nearest-neighbor-search-in-elasticsearch-8-0] For each framework, multiple parameters were used to find the best index in our use case.","title":"ANN Benchmark"},{"location":"research/logo-detection/ann-benchmark/#our-use-case","text":"The data used to test the performance of the indexes was made of 4.371.343 vectors (each one issued from the CLIP vision model with a logo as input) of dimension 768 and type float32. The dimension of each vector used in the benchmark is 768, the size of embeddings CLIP outputs before processing through the projection layer. However, in production, as this last layer of the CLIP model is used, the dimension of the embeddings that are indexed is actually 512. The process to go from 768 to 512 being only a projection, the change of dimensionality should not impact the results. A test of elasticsearch with 512 dimension vectors did indeed lead to very similar results as the ones computed with 768 dimension vectors. The goal is to search for nearest neighbors in real time. The search time must be short. The precision of the search matters as it is important to get logos as close as possible from the query logo for the annotators to be efficient. The feature is expected to work in production on machines that run other tasks in parallel. The memory used by the index and search must thus be as small as possible.","title":"Our Use Case"},{"location":"research/logo-detection/ann-benchmark/#conditions-of-the-tests","text":"All HNSW results were compared to FLAT faiss index search (an exact search) results to compute the precision of the index. Only cosine similarity was usedin this benchmark. HNSW indexes are adjustable trhough three parameters: m: the number of connections that each node in the index has to its neighbors efConstruction: the size of the priority queue used during the construction of the index efSearch/efRuntime/num_candidates: the size of the priority queue used during search operations","title":"Conditions of the tests"},{"location":"research/logo-detection/ann-benchmark/#benchmark","text":"You can find the code of the various benchmarks here . Here are the results obtained with Faiss : m efConstruction efSearch micro-recall@1 micro-recall@4 micro-recall@10 micro-recall@50 micro-recall@100 - - - - - - - - 8 128 64 0.582 0.69475 0.706 0.70754 0.6829 8 128 128 0.591 0.70925 0.7238 0.72874 0.72166 8 256 64 0.598 0.714 0.7265 0.72542 0.70022 8 256 128 0.603 0.72225 0.7356 0.74082 0.7346 16 128 64 0.605 0.72875 0.7393 0.74212 0.72658 16 128 128 0.615 0.737 0.7476 0.75206 0.74833 16 256 64 0.623 0.74175 0.7501 0.75244 0.73805 16 256 128 0.624 0.745 0.7546 0.7581 0.75509 The recall never outreaches 0.76. A better recall is wanted. The Redis HNSW indexes were thus explored. Here are the results obtained with Redis : m efConstruction efRuntime micro-recall@1 micro-recall@4 micro-recall@10 micro-recall@50 micro-recall@100 - - - - - - - - 4 128 64 0.799 0.795 0.7968 0.76916 0.72839 4 128 128 0.822 0.81875 0.8215 0.79918 0.76812 4 256 64 0.83 0.8305 0.8254 0.79126 0.74795 4 256 128 0.855 0.8565 0.8511 0.82204 0.78854 8 128 64 0.955 0.94075 0.9318 0.90614 0.88248 8 128 128 0.964 0.95 0.9403 0.91784 0.90007 8 256 64 0.964 0.9515 0.9424 0.91672 0.89339 8 256 128 0.969 0.95725 0.9477 0.92514 0.90855 The efficiency of these indexes, with the right parameters are better. However, Redis loads all the embeddings in RAM when loading the index, which accounts for more than 20 GB of RAM used by it. This is too much for our use case. The last tool explored is ElasticSearch . With the default parameters, here is what was obtained: m efConstruction num_candidates micro-recall@1 micro-recall@4 micro-recall@10 micro-recall@50 micro-recall@100 - - - - - - - - 16 200 50 0.977 0.9635 0.9533 0.93832 0.93512 The results are excellent. The RAM used is bellow 5 GB. And the time search is around 80 ms, which is acceptable for our use case and way bellow the 3 seconds for exact search with a FLAT faiss index. ElasticSearch was thus chosen for computing the ANN search.","title":"Benchmark"},{"location":"research/logo-detection/embedding-benchmark/","text":"Benchmark When releasing the first version of the logo detection pipeline, the performance was unknown, as we didn't have any labeled logo dataset to measure it. This pipeline helped us build the first annotated logo dataset so that we can now measure how the original EfficientNet-b0 model (pretrained on ImageNet) performs compared to other pretrained models. Logo embeddings were computed using each model. For each logo, two different distances were used to find the most similar logos among the rest of the dataset, and results were sorted by ascending distance. The first table shows the results obtained with the L2 distance and the second one shows the results we got with the cosine distance. To keep the comparison fair and avoid favoring classes with many samples, for each target image, we only considered at most 4 items of each class. These items were sampled at random among the class items. As each class contains at least 5 items, all classes (including the target class, i.e. the class of the target logo) have 4 candidates. With this setting, an oracle model would have a recall@4 of 1. The val split was used to perform this benchmark. The benchmark code can be found here . We use the following metrics: micro-recall@4 : skewed by classes with many samples. macro-recall@4 : gives equal weight to all classes. All compared models were trained on ImageNet, except: beit_large_patch16_224_in22k , trained on ImageNet 22k clip-vit-* , trained on the proprietary dataset described in the CLIP paper . Note that we use the timm library to generate embeddings (except for CLIP models, where the transformers library was used). The model weights mostly come from the timm author's training and differ from the original weights. Latency was measured on 50 batches of 8 samples with a Tesla T4 GPU. With L2 distance : model micro-recall@4 macro-recall@4 random 0.0083 0.0063 efficientnet_b1 0.4612 0.5070 resnest101e 0.4322 0.5124 beit_large_patch16_384 0.4162 0.5233 efficientnet_b2 0.4707 0.5323 rexnet_100 0.5158 0.5340 efficientnet_b4 0.4807 0.5450 resnet50 0.4916 0.5609 efficientnet_b0 0.5420 0.5665 beit_base_patch16_384 0.4758 0.5666 resnet50d 0.5313 0.6133 beit_large_patch16_224_in22k 0.5723 0.6660 clip-vit-base-patch32 0.7006 0.8243 clip-vit-base-patch16 0.7295 0.8428 clip-vit-large-patch14 0.7706 0.8755 deit_base_patch16_384 0.3920 0.4375 With cosine distance : model micro-recall@4 macro-recall@4 random 0.0091 0.0114 efficientnet_b1 0.4941 0.5334 resnest101e 0.4777 0.5415 efficientnet_b2 0.4909 0.5466 rexnet_100 0.5461 0.5716 efficientnet_b4 0.5141 0.5861 resnet50 0.5241 0.5868 efficientnet_b0 0.5489 0.5838 resnet50d 0.5791 0.6353 clip-vit-base-patch32 0.7030 0.8244 clip-vit-base-patch16 0.7297 0.8470 clip-vit-large-patch14 0.7753 0.8722 deit_base_patch16_384 0.4403 0.5070 Embedding size and per-sample latency (independent of the distance used): model embedding size per-sample latency (ms) random - - efficientnet_b1 1280 3.93 resnest101e 1024 142.75 efficientnet_b2 1408 4.29 rexnet_100 1280 3.91 efficientnet_b4 1792 6.99 resnet50 2048 3.50 efficientnet_b0 1280 5.51 beit_base_patch16_384 768 41.88 resnet50d 2048 4.01 beit_large_patch16_224_in22k 1024 43.56 clip-vit-base-patch32 768 3.08 clip-vit-base-patch16 768 11.69 clip-vit-large-patch14 1024 56.68 deit_base_patch16_384 768 1.73 N.B: we didn't use the cosine-distance for the beit models as they were not working anymore when doing the benchmark with the cosine distance. Some explanations can be found there . We note that the cosine distance tends to be slightly better than the L2 distance for each model (except for the clip-vit-large-patch14 ) but the use of one distance or the other does not change the following analysis as the ranking of the models remains the same. As expected, the current model ( efficientnet-b0 ) performs well above the random baseline. Its performances are competitive compared to most other architectures pretrained on ImageNet. However, CLIP models largely outperform any other tested architecture on this benchmark: with clip-vit-large-patch14 we gain +22.8 on micro-recall@4 and +30.9 on macro-recall@4 compared to efficientnet-b0 . Performances of CLIP models increase as models gets larger or with a smaller image patch size. The prediction latency is however 3.8x and 18,4x higher for clip-vit-base-patch16 and clip-vit-large-patch14 respectively compared to clip-vit-base-patch32 . In conclusion, CLIP models are very good candidates for an off-the-shelf replacement of the efficientnet-b0 model currently used to generate logo embeddings. An additional benefit from this model architecture is the smaller embedding size (768 or 1024, depending on the version) compared to the original efficientnet-b0 model (1280). To study the behaviour of each model in a case closer to our usecase, we performed another benchmark based this time of the efficiency of each model to predict the right label for each logo of val.txt file. The predicted label is the most represented one among the k closest neighbours. Here are the results we obtained working with a cosine distance : model micro-recall@4 macro-recall@4 random 0.0098 0.0098 efficientnet_b1 0.7347 0.7711 resnest101e 0.7089 0.7653 efficientnet_b2 0.7379 0.7847 rexnet_100 0.7848 0.7896 efficientnet_b4 0.7590 0.8025 resnet50 0.7667 0.8050 efficientnet_b0 0.7835 0.8098 resnet50d 0.8054 0.8229 clip-vit-base-patch32 0.9139 0.9196 clip-vit-base-patch16 0.9171 0.9219 clip-vit-large-patch14 0.9427 0.9417 deit_base_patch16_384 0.7025 0.7573 The CLIP models remain better than the others.","title":"Benchmark"},{"location":"research/logo-detection/embedding-benchmark/#benchmark","text":"When releasing the first version of the logo detection pipeline, the performance was unknown, as we didn't have any labeled logo dataset to measure it. This pipeline helped us build the first annotated logo dataset so that we can now measure how the original EfficientNet-b0 model (pretrained on ImageNet) performs compared to other pretrained models. Logo embeddings were computed using each model. For each logo, two different distances were used to find the most similar logos among the rest of the dataset, and results were sorted by ascending distance. The first table shows the results obtained with the L2 distance and the second one shows the results we got with the cosine distance. To keep the comparison fair and avoid favoring classes with many samples, for each target image, we only considered at most 4 items of each class. These items were sampled at random among the class items. As each class contains at least 5 items, all classes (including the target class, i.e. the class of the target logo) have 4 candidates. With this setting, an oracle model would have a recall@4 of 1. The val split was used to perform this benchmark. The benchmark code can be found here . We use the following metrics: micro-recall@4 : skewed by classes with many samples. macro-recall@4 : gives equal weight to all classes. All compared models were trained on ImageNet, except: beit_large_patch16_224_in22k , trained on ImageNet 22k clip-vit-* , trained on the proprietary dataset described in the CLIP paper . Note that we use the timm library to generate embeddings (except for CLIP models, where the transformers library was used). The model weights mostly come from the timm author's training and differ from the original weights. Latency was measured on 50 batches of 8 samples with a Tesla T4 GPU. With L2 distance : model micro-recall@4 macro-recall@4 random 0.0083 0.0063 efficientnet_b1 0.4612 0.5070 resnest101e 0.4322 0.5124 beit_large_patch16_384 0.4162 0.5233 efficientnet_b2 0.4707 0.5323 rexnet_100 0.5158 0.5340 efficientnet_b4 0.4807 0.5450 resnet50 0.4916 0.5609 efficientnet_b0 0.5420 0.5665 beit_base_patch16_384 0.4758 0.5666 resnet50d 0.5313 0.6133 beit_large_patch16_224_in22k 0.5723 0.6660 clip-vit-base-patch32 0.7006 0.8243 clip-vit-base-patch16 0.7295 0.8428 clip-vit-large-patch14 0.7706 0.8755 deit_base_patch16_384 0.3920 0.4375 With cosine distance : model micro-recall@4 macro-recall@4 random 0.0091 0.0114 efficientnet_b1 0.4941 0.5334 resnest101e 0.4777 0.5415 efficientnet_b2 0.4909 0.5466 rexnet_100 0.5461 0.5716 efficientnet_b4 0.5141 0.5861 resnet50 0.5241 0.5868 efficientnet_b0 0.5489 0.5838 resnet50d 0.5791 0.6353 clip-vit-base-patch32 0.7030 0.8244 clip-vit-base-patch16 0.7297 0.8470 clip-vit-large-patch14 0.7753 0.8722 deit_base_patch16_384 0.4403 0.5070 Embedding size and per-sample latency (independent of the distance used): model embedding size per-sample latency (ms) random - - efficientnet_b1 1280 3.93 resnest101e 1024 142.75 efficientnet_b2 1408 4.29 rexnet_100 1280 3.91 efficientnet_b4 1792 6.99 resnet50 2048 3.50 efficientnet_b0 1280 5.51 beit_base_patch16_384 768 41.88 resnet50d 2048 4.01 beit_large_patch16_224_in22k 1024 43.56 clip-vit-base-patch32 768 3.08 clip-vit-base-patch16 768 11.69 clip-vit-large-patch14 1024 56.68 deit_base_patch16_384 768 1.73 N.B: we didn't use the cosine-distance for the beit models as they were not working anymore when doing the benchmark with the cosine distance. Some explanations can be found there . We note that the cosine distance tends to be slightly better than the L2 distance for each model (except for the clip-vit-large-patch14 ) but the use of one distance or the other does not change the following analysis as the ranking of the models remains the same. As expected, the current model ( efficientnet-b0 ) performs well above the random baseline. Its performances are competitive compared to most other architectures pretrained on ImageNet. However, CLIP models largely outperform any other tested architecture on this benchmark: with clip-vit-large-patch14 we gain +22.8 on micro-recall@4 and +30.9 on macro-recall@4 compared to efficientnet-b0 . Performances of CLIP models increase as models gets larger or with a smaller image patch size. The prediction latency is however 3.8x and 18,4x higher for clip-vit-base-patch16 and clip-vit-large-patch14 respectively compared to clip-vit-base-patch32 . In conclusion, CLIP models are very good candidates for an off-the-shelf replacement of the efficientnet-b0 model currently used to generate logo embeddings. An additional benefit from this model architecture is the smaller embedding size (768 or 1024, depending on the version) compared to the original efficientnet-b0 model (1280). To study the behaviour of each model in a case closer to our usecase, we performed another benchmark based this time of the efficiency of each model to predict the right label for each logo of val.txt file. The predicted label is the most represented one among the k closest neighbours. Here are the results we obtained working with a cosine distance : model micro-recall@4 macro-recall@4 random 0.0098 0.0098 efficientnet_b1 0.7347 0.7711 resnest101e 0.7089 0.7653 efficientnet_b2 0.7379 0.7847 rexnet_100 0.7848 0.7896 efficientnet_b4 0.7590 0.8025 resnet50 0.7667 0.8050 efficientnet_b0 0.7835 0.8098 resnet50d 0.8054 0.8229 clip-vit-base-patch32 0.9139 0.9196 clip-vit-base-patch16 0.9171 0.9219 clip-vit-large-patch14 0.9427 0.9417 deit_base_patch16_384 0.7025 0.7573 The CLIP models remain better than the others.","title":"Benchmark"},{"location":"research/nutrition-table-detection/","text":"Overview The annotation guideline that was used to annotate the nutrition table can be found here .","title":"Overview"},{"location":"research/nutrition-table-detection/#overview","text":"The annotation guideline that was used to annotate the nutrition table can be found here .","title":"Overview"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/","text":"Nutrition Table Annotation Guidelines Guidelines on what and how to label. Adapted from http://host.robots.ox.ac.uk/pascal/VOC/voc2011/guidelines.html What to label All objects of the defined categories, unless: you are unsure what the object is. the object is very small (at your discretion). less than 10-20% of the object is visible, such that you cannot be sure what class it is. Bounding box Mark the bounding box of the visible area of the object (not the estimated total extent of the object). Bounding box should contain all visible pixels. The bounding box should enclose the object as tight as possible. Clothing/mud/ snow etc. If an object is \u2018occluded\u2019 by a close-fitting occluder e.g. clothing, mud, snow etc., then the occluder should be treated as part of the object. Transparency Do label objects visible through glass, but treat reflections on the glass as occlusion. Mirrors Do label objects in mirrors. Pictures Label objects in pictures/posters/signs only if they are photorealistic but not if cartoons, symbols etc. Guidelines on categorization nutrition-table : a table containing nutrition facts. nutrition-table-text : variant where the nutrition facts are not displayed in a table. Ex: Nutritional facts for 100g: Energy - 252 kJ, fat: 12g,... . nutrition-table-small-energy : symbol often found on the front image of the product, indicating the kJ/kcal of a portion/100g of the product. The bounding box should only enclose the symbol, and not additional texts around it. Do not use this label if other nutritional information are layed out next to the object, see nutrition-table-small. nutrition-table-small : pack of symbols often found on the front image of the product, indicating the nutrition facts. If there are several nutrition-table or nutrition-table-text on the image (often found on multilingual products), label each object.","title":"Nutrition Table Annotation Guidelines"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#nutrition-table-annotation-guidelines","text":"Guidelines on what and how to label. Adapted from http://host.robots.ox.ac.uk/pascal/VOC/voc2011/guidelines.html","title":"Nutrition Table Annotation Guidelines"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#what-to-label","text":"All objects of the defined categories, unless: you are unsure what the object is. the object is very small (at your discretion). less than 10-20% of the object is visible, such that you cannot be sure what class it is.","title":"What to label"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#bounding-box","text":"Mark the bounding box of the visible area of the object (not the estimated total extent of the object). Bounding box should contain all visible pixels. The bounding box should enclose the object as tight as possible.","title":"Bounding box"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#clothingmud-snow-etc","text":"If an object is \u2018occluded\u2019 by a close-fitting occluder e.g. clothing, mud, snow etc., then the occluder should be treated as part of the object.","title":"Clothing/mud/ snow etc."},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#transparency","text":"Do label objects visible through glass, but treat reflections on the glass as occlusion.","title":"Transparency"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#mirrors","text":"Do label objects in mirrors.","title":"Mirrors"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#pictures","text":"Label objects in pictures/posters/signs only if they are photorealistic but not if cartoons, symbols etc.","title":"Pictures"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#guidelines-on-categorization","text":"nutrition-table : a table containing nutrition facts. nutrition-table-text : variant where the nutrition facts are not displayed in a table. Ex: Nutritional facts for 100g: Energy - 252 kJ, fat: 12g,... . nutrition-table-small-energy : symbol often found on the front image of the product, indicating the kJ/kcal of a portion/100g of the product. The bounding box should only enclose the symbol, and not additional texts around it. Do not use this label if other nutritional information are layed out next to the object, see nutrition-table-small. nutrition-table-small : pack of symbols often found on the front image of the product, indicating the nutrition facts. If there are several nutrition-table or nutrition-table-text on the image (often found on multilingual products), label each object.","title":"Guidelines on categorization"}]}