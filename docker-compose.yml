x-robotoff-base-volumes:
  &robotoff-base-volumes
  - ./data:/opt/robotoff/data
  - ./cache:/opt/robotoff/cache
  - ./datasets:/opt/robotoff/datasets
  - ./models:/opt/robotoff/models
  - robotoff_tmp:/tmp

x-robotoff-base:
  &robotoff-base
  restart: $RESTART_POLICY
  # this image is built with target=runtime, so it won't have the dev dependencies
  image: ghcr.io/openfoodfacts/robotoff:${TAG}
  volumes: *robotoff-base-volumes
  networks:
    - default
    - common_net

x-robotoff-base-env:
  &robotoff-base-env
  LOG_LEVEL:
  ROBOTOFF_INSTANCE:
  ROBOTOFF_TLD:
  ROBOTOFF_SCHEME:
  STATIC_DOMAIN:
  GUNICORN_PRELOAD_APP:
  GUNICORN_NUM_WORKERS:
  ROBOTOFF_UPDATED_PRODUCT_WAIT:
  REDIS_HOST:
  REDIS_UPDATE_HOST:
  REDIS_UPDATE_PORT:
  REDIS_STREAM_NAME:
  POSTGRES_HOST:
  POSTGRES_DB:
  POSTGRES_USER:
  POSTGRES_PASSWORD:
  MONGO_URI:
  OFF_USER:
  OFF_PASSWORD:
  INFLUXDB_HOST:
  INFLUXDB_PORT:
  INFLUXDB_BUCKET:
  INFLUXDB_AUTH_TOKEN:
  SLACK_TOKEN:
  SENTRY_DSN:
  ELASTIC_HOST:
  ELASTIC_PASSWORD:
  ELASTIC_USER:
  TRITON_HOST:
  FASTTEXT_HOST:
  FASTTEXT_PORT:
  ENABLE_MONGODB_ACCESS:
  IN_DOCKER_CONTAINER:
  IMAGE_MODERATION_SERVICE_URL:
  CROP_ALLOWED_DOMAINS:
  NUM_RQ_WORKERS: 4 # Update worker service command accordingly if you change this settings
  GOOGLE_CREDENTIALS: # JSON credentials pasted as environment variable
  BATCH_JOB_KEY: # Secure Batch job import with a token key 
  
x-robotoff-worker-base:
  &robotoff-worker
  restart: $RESTART_POLICY
  image: ghcr.io/openfoodfacts/robotoff:${TAG}
  volumes: *robotoff-base-volumes
  environment: *robotoff-base-env
  depends_on:
    - postgres
    - redis
  mem_limit: 8g
  networks:
    - default
    - common_net

services:
  api:
    <<: *robotoff-base
    environment: *robotoff-base-env
    mem_limit: 6g
    ports:
      - "${ROBOTOFF_EXPOSE:-5500}:5500"
    depends_on:
      - postgres
      - redis
      - elasticsearch

  update-listener:
    <<: *robotoff-base
    environment: *robotoff-base-env
    mem_limit: 1g
    depends_on:
      - redis
    command: python -m robotoff run-update-listener

  worker_1:
    <<: *robotoff-worker
    container_name: ${COMPOSE_PROJECT_NAME:-robotoff}_worker_1
    # Each worker listens to a single high priority queue and to the low priority queue.
    # As the low priority queue comes last, it will only be processed if there are no high
    # priority tasks.
    command: python -m robotoff run-worker robotoff-high-1 robotoff-low

  worker_2:
    <<: *robotoff-worker
    container_name: ${COMPOSE_PROJECT_NAME:-robotoff}_worker_2
    command: python -m robotoff run-worker robotoff-high-2 robotoff-low

  worker_3:
    <<: *robotoff-worker
    container_name: ${COMPOSE_PROJECT_NAME:-robotoff}_worker_3
    command: python -m robotoff run-worker robotoff-high-3 robotoff-low

  worker_4:
    <<: *robotoff-worker
    container_name: ${COMPOSE_PROJECT_NAME:-robotoff}_worker_4
    command: python -m robotoff run-worker robotoff-high-4 robotoff-low

  scheduler:
    <<: *robotoff-base
    environment: *robotoff-base-env
    command: python -m robotoff run-scheduler
    mem_limit: 4g

  postgres:
    restart: $RESTART_POLICY
    image: postgres:16.3-alpine
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - backup:/opt/robotoff-backups
      - ./scripts/backup_postgres.sh:/opt/backup_postgres.sh
    command: postgres -c shared_buffers=${ROBOTOFF_POSTGRES_SHARED_BUFFERS} -c work_mem=${ROBOTOFF_POSTGRES_WORK_MEM}
    mem_limit: 20g
    shm_size: 1g
    ports:
      - "${POSTGRES_EXPOSE:-127.0.0.1:5432}:5432"

  redis:
    restart: $RESTART_POLICY
    image: redis:7.0.5-alpine
    volumes:
      - redis-data:/data
    environment:
      REDIS_ARGS: --save 60 1000 --appendonly yes
    mem_limit: 4g
    ports:
      - "${REDIS_EXPOSE:-127.0.0.1:6379}:6379"

  elasticsearch:
    restart: $RESTART_POLICY
    image: elasticsearch:8.5.3
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - ELASTIC_PASSWORD
      - xpack.security.http.ssl.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 262144
        hard: 262144
    mem_limit: 15g
    volumes:
      - es-data:/usr/share/elasticsearch/data
      - elasticsearch_tmp:/tmp

volumes:
  postgres-data:
    name: ${COMPOSE_PROJECT_NAME:-robotoff}_postgres-data
  es-data:
    name: ${COMPOSE_PROJECT_NAME:-robotoff}_es-data
  redis-data:
    name: ${COMPOSE_PROJECT_NAME:-robotoff}_redis-data
  backup:
    name: ${COMPOSE_PROJECT_NAME:-robotoff}_backup
  # Volume mount on /tmp on API and workers, to prevent
  # large docker layer overlay
  robotoff_tmp:
    name: ${COMPOSE_PROJECT_NAME:-robotoff}_tmp
  elasticsearch_tmp:
    name: ${COMPOSE_PROJECT_NAME:-robotoff}_elasticsearch_tmp


networks:
  default:

  # this is the network shared with product opener
  common_net:
    name: ${COMMON_NET_NAME}
    external: true
