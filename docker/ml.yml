version: "3.9"

services:
  triton:
    restart: $RESTART_POLICY
    # This is a custom built of Triton with:
    # - GRPC/HTTP support
    # - CPU only (we don't have GPU in production)
    # - Tensorflow 2 SavedModel and ONNX support
    # This allows us to reduce significantly the image size
    # See https://gist.github.com/raphael0202/091e521f2c79a8db8c6e9aceafb6e0b9 for build script
    image: ghcr.io/openfoodfacts/triton:cpu
    ports:
      - ${TRITON_EXPOSE_HTTP:-8000}:8000
      - ${TRITON_EXPOSE_GRPC:-8001}:8001
      - ${TRITON_EXPOSE_METRICS:-8002}:8002
    volumes:
      - ${TRITON_MODELS_DIR:-../models/triton}:/models
    # We use explicit model control mode to be able to load/unload model dynamically
    # without having to restart Triton server
    # See
    # https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md
    # for more information
    entrypoint: "tritonserver --model-repository=/models --model-control-mode=explicit --load-model=*"
    mem_limit: 15g

  fasttext:
    restart: $RESTART_POLICY
    image: messense/fasttext-serving
    ports:
      - ${FASTTEXT_EXPOSE:-8020}:8000
    volumes:
      - ${FASTTEXT_MODEL_DIR:-../models}:/models
    entrypoint: "fasttext-serving --model=/models/lid.176.bin --port 8000 --address 0.0.0.0"