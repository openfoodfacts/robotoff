services:
  triton:
    restart: $RESTART_POLICY
    image: nvcr.io/nvidia/tritonserver:24.08-py3
    ports:
      - ${TRITON_EXPOSE_HTTP:-8000}:8000
      - ${TRITON_EXPOSE_GRPC:-8001}:8001
      - ${TRITON_EXPOSE_METRICS:-8002}:8002
    volumes:
      - ${TRITON_MODELS_DIR:-../models/triton}:/models
    # We use explicit model control mode to be able to load/unload model dynamically
    # without having to restart Triton server
    # See
    # https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md
    # for more information
    entrypoint: "tritonserver --model-repository=/models --model-control-mode=explicit --load-model=*"
    mem_limit: 15g

  fasttext:
    restart: $RESTART_POLICY
    image: messense/fasttext-serving
    ports:
      - ${FASTTEXT_EXPOSE:-8020}:8000
    volumes:
      - ${FASTTEXT_MODEL_DIR:-../models}:/models
    entrypoint: "fasttext-serving --model=/models/lid.176.bin --port 8000 --address 0.0.0.0"